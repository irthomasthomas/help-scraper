GET-PERSON-TRACKING()                                    GET-PERSON-TRACKING()



NAME
       get-person-tracking -

DESCRIPTION
       Gets  the  path tracking results of a Amazon Rekognition Video analysis
       started by  StartPersonTracking .

       The person path tracking operation is started by a call to StartPerson-
       Tracking  which  returns  a job identifier (JobId ). When the operation
       finishes, Amazon Rekognition Video publishes a completion status to the
       Amazon Simple Notification Service topic registered in the initial call
       to StartPersonTracking .

       To get the results of the person path tracking operation,  first  check
       that  the status value published to the Amazon SNS topic is SUCCEEDED .
       If so, call  GetPersonTracking and pass the  job  identifier  (JobId  )
       from the initial call to StartPersonTracking .
          GetPersonTracking returns an array, Persons , of tracked persons and
          the time(s) their paths were tracked in the video.

       NOTE:
              GetPersonTracking only returns  the  default  facial  attributes
              (BoundingBox  ,  Confidence  , Landmarks , Pose , and Quality ).
              The other facial attributes listed in the  Face  object  of  the
              following response syntax are not returned.

          For  more  information, see FaceDetail in the Amazon Rekognition De-
          veloper Guide.

       By default, the array is sorted by  the  time(s)  a  person's  path  is
       tracked in the video. You can sort by tracked persons by specifying IN-
       DEX for the SortBy input parameter.

       Use the MaxResults parameter to limit the number of items returned.  If
       there  are  more  results  than  specified in MaxResults , the value of
       NextToken in the operation response contains  a  pagination  token  for
       getting  the next set of results. To get the next page of results, call
       GetPersonTracking and populate the NextToken request parameter with the
       token value returned from the previous call to GetPersonTracking .

       See also: AWS API Documentation

SYNOPSIS
            get-person-tracking
          --job-id <value>
          [--max-results <value>]
          [--next-token <value>]
          [--sort-by <value>]
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]
          [--debug]
          [--endpoint-url <value>]
          [--no-verify-ssl]
          [--no-paginate]
          [--output <value>]
          [--query <value>]
          [--profile <value>]
          [--region <value>]
          [--version <value>]
          [--color <value>]
          [--no-sign-request]
          [--ca-bundle <value>]
          [--cli-read-timeout <value>]
          [--cli-connect-timeout <value>]

OPTIONS
       --job-id (string)
          The identifier for a job that tracks persons in a video. You get the
          JobId from a call to StartPersonTracking .

       --max-results (integer)
          Maximum number of results to return per paginated call. The  largest
          value  you  can specify is 1000. If you specify a value greater than
          1000, a maximum of 1000 results is returned. The  default  value  is
          1000.

       --next-token (string)
          If the previous response was incomplete (because there are more per-
          sons to retrieve), Amazon Rekognition Video returns a pagination to-
          ken  in  the response. You can use this pagination token to retrieve
          the next set of persons.

       --sort-by (string)
          Sort to use for elements in the Persons array. Use TIMESTAMP to sort
          array  elements  by the time persons are detected. Use INDEX to sort
          by the tracked persons. If you sort by INDEX ,  the  array  elements
          for each person are sorted by detection confidence. The default sort
          is by TIMESTAMP .

          Possible values:

          o INDEX

          o TIMESTAMP

       --cli-input-json (string) Performs service operation based on the  JSON
       string  provided. The JSON string follows the format provided by --gen-
       erate-cli-skeleton. If other arguments  are  provided  on  the  command
       line,  the CLI values will override the JSON-provided values. It is not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton  (string)  Prints  a  JSON skeleton to standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for --cli-input-json. If provided with the value output,  it  validates
       the command inputs and returns a sample output JSON for that command.

GLOBAL OPTIONS
       --debug (boolean)

       Turn on debug logging.

       --endpoint-url (string)

       Override command's default URL with the given URL.

       --no-verify-ssl (boolean)

       By  default, the AWS CLI uses SSL when communicating with AWS services.
       For each SSL connection, the AWS CLI will verify SSL certificates. This
       option overrides the default behavior of verifying SSL certificates.

       --no-paginate (boolean)

       Disable automatic pagination.

       --output (string)

       The formatting style for command output.

       o json

       o text

       o table

       --query (string)

       A JMESPath query to use in filtering the response data.

       --profile (string)

       Use a specific profile from your credential file.

       --region (string)

       The region to use. Overrides config/env settings.

       --version (string)

       Display the version of this tool.

       --color (string)

       Turn on/off color output.

       o on

       o off

       o auto

       --no-sign-request (boolean)

       Do  not  sign requests. Credentials will not be loaded if this argument
       is provided.

       --ca-bundle (string)

       The CA certificate bundle to use when verifying SSL certificates. Over-
       rides config/env settings.

       --cli-read-timeout (int)

       The  maximum socket read time in seconds. If the value is set to 0, the
       socket read will be blocking and not timeout. The default value  is  60
       seconds.

       --cli-connect-timeout (int)

       The  maximum  socket connect time in seconds. If the value is set to 0,
       the socket connect will be blocking and not timeout. The default  value
       is 60 seconds.

EXAMPLES
       NOTE:
          To  use  the following examples, you must have the AWS CLI installed
          and configured. See the Getting started guide in the  AWS  CLI  User
          Guide for more information.

          Unless  otherwise  stated,  all  examples  have  unix-like quotation
          rules. These examples will need to be  adapted  to  your  terminal's
          quoting rules. See Using quotation marks with strings in the AWS CLI
          User Guide .

       To get the results of a people pathing operation

       The following get-person-tracking command displays  the  results  of  a
       people  pathing  operation  that  you  started  previously  by  calling
       start-person-tracking.

          aws rekognition get-person-tracking  \
              --job-id 1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef

       Output:

          {
              "Persons": [
                  {
                      "Timestamp": 500,
                      "Person": {
                          "BoundingBox": {
                              "Width": 0.4151041805744171,
                              "Top": 0.07870370149612427,
                              "Left": 0.0,
                              "Height": 0.9212962985038757
                          },
                          "Index": 0
                      }
                  },
                  {
                      "Timestamp": 567,
                      "Person": {
                          "BoundingBox": {
                              "Width": 0.4755208194255829,
                              "Top": 0.07777778059244156,
                              "Left": 0.0,
                              "Height": 0.9194444417953491
                          },
                          "Index": 0
                      }
                  }
              ],
              "NextToken": "D/vRIYNyhG79ugdta3f+8cRg9oSRo+HigGOuxRiYpTn0ExnqTi1CJektVAc4HrAXDv25eHYk",
              "JobStatus": "SUCCEEDED",
              "VideoMetadata": {
                  "Format": "QuickTime / MOV",
                  "FrameRate": 29.970617294311523,
                  "Codec": "h264",
                  "DurationMillis": 6806,
                  "FrameHeight": 1080,
                  "FrameWidth": 1920
              }
          }

       For more information, see People Pathing in the Amazon Rekognition  De-
       veloper Guide.

OUTPUT
       JobStatus -> (string)
          The current status of the person tracking job.

       StatusMessage -> (string)
          If  the  job  fails, StatusMessage provides a descriptive error mes-
          sage.

       VideoMetadata -> (structure)
          Information about a video that Amazon  Rekognition  Video  analyzed.
          Videometadata  is returned in every page of paginated responses from
          a Amazon Rekognition Video operation.

          Codec -> (string)
              Type of compression used in the analyzed video.

          DurationMillis -> (long)
              Length of the video in milliseconds.

          Format -> (string)
              Format of the analyzed video. Possible values are MP4,  MOV  and
              AVI.

          FrameRate -> (float)
              Number of frames per second in the video.

          FrameHeight -> (long)
              Vertical pixel dimension of the video.

          FrameWidth -> (long)
              Horizontal pixel dimension of the video.

          ColorRange -> (string)
              A  description  of the range of luminance values in a video, ei-
              ther LIMITED (16 to 235) or FULL (0 to 255).

       NextToken -> (string)
          If the response is truncated, Amazon Rekognition Video returns  this
          token  that  you  can  use in the subsequent request to retrieve the
          next set of persons.

       Persons -> (list)
          An array of the persons detected in the video and the time(s)  their
          path  was  tracked throughout the video. An array element will exist
          for each time a person's path is tracked.

          (structure)
              Details and path tracking information for a single time  a  per-
              son's  path is tracked in a video. Amazon Rekognition operations
              that track people's paths return an array of PersonDetection ob-
              jects  with elements for each time a person's path is tracked in
              a video.

              For more information, see GetPersonTracking in the Amazon Rekog-
              nition Developer Guide.

              Timestamp -> (long)
                 The  time,  in milliseconds from the start of the video, that
                 the person's path was tracked. Note  that  Timestamp  is  not
                 guaranteed  to  be accurate to the individual frame where the
                 person's path first appears.

              Person -> (structure)
                 Details about a person whose path was tracked in a video.

                 Index -> (long)
                     Identifier for the person detected person within a video.
                     Use to keep track of the person throughout the video. The
                     identifier is not stored by Amazon Rekognition.

                 BoundingBox -> (structure)
                     Bounding box around the detected person.

                     Width -> (float)
                        Width of the bounding box as a ratio  of  the  overall
                        image width.

                     Height -> (float)
                        Height  of  the bounding box as a ratio of the overall
                        image height.

                     Left -> (float)
                        Left coordinate of the bounding  box  as  a  ratio  of
                        overall image width.

                     Top -> (float)
                        Top coordinate of the bounding box as a ratio of over-
                        all image height.

                 Face -> (structure)
                     Face details for the detected person.

                     BoundingBox -> (structure)
                        Bounding box of the face. Default attribute.

                        Width -> (float)
                            Width of the bounding box as a ratio of the  over-
                            all image width.

                        Height -> (float)
                            Height of the bounding box as a ratio of the over-
                            all image height.

                        Left -> (float)
                            Left coordinate of the bounding box as a ratio  of
                            overall image width.

                        Top -> (float)
                            Top  coordinate  of the bounding box as a ratio of
                            overall image height.

                     AgeRange -> (structure)
                        The estimated age range, in years, for the  face.  Low
                        represents  the  lowest  estimated age and High repre-
                        sents the highest estimated age.

                        Low -> (integer)
                            The lowest estimated age.

                        High -> (integer)
                            The highest estimated age.

                     Smile -> (structure)
                        Indicates whether or not the face is smiling, and  the
                        confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value  that indicates whether the face is
                            smiling or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Eyeglasses -> (structure)
                        Indicates whether or  not  the  face  is  wearing  eye
                        glasses,  and  the  confidence level in the determina-
                        tion.

                        Value -> (boolean)
                            Boolean value that indicates whether the  face  is
                            wearing eye glasses or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Sunglasses -> (structure)
                        Indicates  whether  or  not  the  face is wearing sun-
                        glasses, and the confidence level  in  the  determina-
                        tion.

                        Value -> (boolean)
                            Boolean  value  that indicates whether the face is
                            wearing sunglasses or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Gender -> (structure)
                        The predicted gender of a detected face.

                        Value -> (string)
                            The predicted gender of the face.

                        Confidence -> (float)
                            Level of confidence in the prediction.

                     Beard -> (structure)
                        Indicates whether or not the face has a beard, and the
                        confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value that indicates whether the face has
                            beard or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Mustache -> (structure)
                        Indicates whether or not the face has a mustache,  and
                        the confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value that indicates whether the face has
                            mustache or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     EyesOpen -> (structure)
                        Indicates whether or not the  eyes  on  the  face  are
                        open, and the confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value  that indicates whether the eyes on
                            the face are open.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     MouthOpen -> (structure)
                        Indicates whether or not the  mouth  on  the  face  is
                        open, and the confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value that indicates whether the mouth on
                            the face is open or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Emotions -> (list)
                        The emotions that appear to be expressed on the  face,
                        and the confidence level in the determination. The API
                        is only making a determination of the physical appear-
                        ance  of a person's face. It is not a determination of
                        the persons internal emotional state and should not be
                        used  in  such a way. For example, a person pretending
                        to have a sad face might not be sad emotionally.

                        (structure)
                            The emotions that appear to be  expressed  on  the
                            face,  and  the confidence level in the determina-
                            tion. The API is only making  a  determination  of
                            the  physical appearance of a person's face. It is
                            not a determination of the persons  internal  emo-
                            tional state and should not be used in such a way.
                            For example, a person pretending  to  have  a  sad
                            face might not be sad emotionally.

                            Type -> (string)
                               Type of emotion detected.

                            Confidence -> (float)
                               Level of confidence in the determination.

                     Landmarks -> (list)
                        Indicates  the  location of landmarks on the face. De-
                        fault attribute.

                        (structure)
                            Indicates the location  of  the  landmark  on  the
                            face.

                            Type -> (string)
                               Type of landmark.

                            X -> (float)
                               The x-coordinate of the landmark expressed as a
                               ratio of the width of the image. The  x-coordi-
                               nate  is measured from the left-side of the im-
                               age. For example, if the image  is  700  pixels
                               wide and the x-coordinate of the landmark is at
                               350 pixels, this value is 0.5.

                            Y -> (float)
                               The y-coordinate of the landmark expressed as a
                               ratio of the height of the image. The y-coordi-
                               nate is measured from the top of the image. For
                               example,  if the image height is 200 pixels and
                               the y-coordinate of the landmark is at 50  pix-
                               els, this value is 0.25.

                     Pose -> (structure)
                        Indicates  the  pose  of the face as determined by its
                        pitch, roll, and yaw. Default attribute.

                        Roll -> (float)
                            Value representing the face rotation on  the  roll
                            axis.

                        Yaw -> (float)
                            Value  representing  the  face rotation on the yaw
                            axis.

                        Pitch -> (float)
                            Value representing the face rotation on the  pitch
                            axis.

                     Quality -> (structure)
                        Identifies image brightness and sharpness. Default at-
                        tribute.

                        Brightness -> (float)
                            Value representing brightness  of  the  face.  The
                            service  returns a value between 0 and 100 (inclu-
                            sive). A higher value indicates  a  brighter  face
                            image.

                        Sharpness -> (float)
                            Value representing sharpness of the face. The ser-
                            vice returns a value between  0  and  100  (inclu-
                            sive). A higher value indicates a sharper face im-
                            age.

                     Confidence -> (float)
                        Confidence level that the bounding box contains a face
                        (and  not  a different object such as a tree). Default
                        attribute.

                     FaceOccluded -> (structure)
                            FaceOccluded should return "true" with a high con-
                            fidence  score if a detected faces eyes, nose, and
                            mouth are partially captured or if they  are  cov-
                            ered  by  masks,  dark  sunglasses,  cell  phones,
                            hands, or other objects. FaceOccluded  should  re-
                            turn  "false" with a high confidence score if com-
                            mon occurrences that do not impact face  verifica-
                            tion  are  detected,  such as eye glasses, lightly
                            tinted sunglasses, strands of hair, and others.

                        Value -> (boolean)
                            True if a detected faces eyes, nose, and mouth are
                            partially  captured  or  if  they  are  covered by
                            masks, dark sunglasses,  cell  phones,  hands,  or
                            other objects. False if common occurrences that do
                            not impact face verification are detected, such as
                            eye glasses, lightly tinted sunglasses, strands of
                            hair, and others.

                        Confidence -> (float)
                            The confidence that the service has  detected  the
                            presence of a face occlusion.

                     EyeDirection -> (structure)
                        Indicates the direction the eyes are gazing in, as de-
                        fined by pitch and yaw.

                        Yaw -> (float)
                            Value representing eye direction on the yaw axis.

                        Pitch -> (float)
                            Value representing  eye  direction  on  the  pitch
                            axis.

                        Confidence -> (float)
                            The  confidence  that  the service has in its pre-
                            dicted eye direction.

       JobId -> (string)
          Job identifier for the person tracking operation for which you  want
          to  obtain results. The job identifer is returned by an initial call
          to StartPersonTracking.

       Video -> (structure)
          Video file stored in an Amazon S3 bucket. Amazon  Rekognition  video
          start operations such as  StartLabelDetection use Video to specify a
          video for analysis. The supported file formats are  .mp4,  .mov  and
          .avi.

          S3Object -> (structure)
              The Amazon S3 bucket name and file name for the video.

              Bucket -> (string)
                 Name of the S3 bucket.

              Name -> (string)
                 S3 object key name.

              Version -> (string)
                 If  the bucket is versioning enabled, you can specify the ob-
                 ject version.

       JobTag -> (string)
          A job identifier specified in the call to  StartCelebrityRecognition
          and  returned in the job completion notification sent to your Amazon
          Simple Notification Service topic.



                                                         GET-PERSON-TRACKING()
