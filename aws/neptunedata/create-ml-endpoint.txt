CREATE-ML-ENDPOINT()                                      CREATE-ML-ENDPOINT()



NAME
       create-ml-endpoint -

DESCRIPTION
       Creates  a  new  Neptune  ML inference endpoint that lets you query one
       specific  model  that  the  model-training  process  constructed.   See
       Managing inference endpoints using the endpoints command .

       When  invoking this operation in a Neptune cluster that has IAM authen-
       tication enabled, the IAM user or role making the request must  have  a
       policy  attached that allows the neptune-db:CreateMLEndpoint IAM action
       in that cluster.

       See also: AWS API Documentation

SYNOPSIS
            create-ml-endpoint
          [--id <value>]
          [--ml-model-training-job-id <value>]
          [--ml-model-transform-job-id <value>]
          [--update | --no-update]
          [--neptune-iam-role-arn <value>]
          [--model-name <value>]
          [--instance-type <value>]
          [--instance-count <value>]
          [--volume-encryption-kms-key <value>]
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]
          [--debug]
          [--endpoint-url <value>]
          [--no-verify-ssl]
          [--no-paginate]
          [--output <value>]
          [--query <value>]
          [--profile <value>]
          [--region <value>]
          [--version <value>]
          [--color <value>]
          [--no-sign-request]
          [--ca-bundle <value>]
          [--cli-read-timeout <value>]
          [--cli-connect-timeout <value>]

OPTIONS
       --id (string)
          A unique identifier for the new inference endpoint. The  default  is
          an autogenerated timestamped name.

       --ml-model-training-job-id (string)
          The  job Id of the completed model-training job that has created the
          model that the inference endpoint will point to. You must supply ei-
          ther the mlModelTrainingJobId or the mlModelTransformJobId .

       --ml-model-transform-job-id (string)
          The job Id of the completed model-transform job. You must supply ei-
          ther the mlModelTrainingJobId or the mlModelTransformJobId .

       --update | --no-update (boolean)
          If set to true , update indicates that this is  an  update  request.
          The default is false . You must supply either the mlModelTrainingJo-
          bId or the mlModelTransformJobId .

       --neptune-iam-role-arn (string)
          The ARN of an IAM role providing Neptune  access  to  SageMaker  and
          Amazon  S3 resources. This must be listed in your DB cluster parame-
          ter group or an error will be thrown.

       --model-name (string)
          Model type for training. By default the Neptune ML model is automat-
          ically  based  on the modelType used in data processing, but you can
          specify a different model type here. The default is rgcn for hetero-
          geneous  graphs  and  kge for knowledge graphs. The only valid value
          for heterogeneous graphs is rgcn . Valid values for knowledge graphs
          are: kge , transe , distmult , and rotate .

       --instance-type (string)
          The type of Neptune ML instance to use for online servicing. The de-
          fault is ml.m5.xlarge . Choosing the ML instance  for  an  inference
          endpoint depends on the task type, the graph size, and your budget.

       --instance-count (integer)
          The  minimum number of Amazon EC2 instances to deploy to an endpoint
          for prediction. The default is 1

       --volume-encryption-kms-key (string)
          The Amazon Key Management Service (Amazon KMS)  key  that  SageMaker
          uses  to  encrypt data on the storage volume attached to the ML com-
          pute instances that run the training job. The default is None.

       --cli-input-json (string) Performs service operation based on the  JSON
       string  provided. The JSON string follows the format provided by --gen-
       erate-cli-skeleton. If other arguments  are  provided  on  the  command
       line,  the CLI values will override the JSON-provided values. It is not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton  (string)  Prints  a  JSON skeleton to standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for --cli-input-json. If provided with the value output,  it  validates
       the command inputs and returns a sample output JSON for that command.

GLOBAL OPTIONS
       --debug (boolean)

       Turn on debug logging.

       --endpoint-url (string)

       Override command's default URL with the given URL.

       --no-verify-ssl (boolean)

       By  default, the AWS CLI uses SSL when communicating with AWS services.
       For each SSL connection, the AWS CLI will verify SSL certificates. This
       option overrides the default behavior of verifying SSL certificates.

       --no-paginate (boolean)

       Disable automatic pagination.

       --output (string)

       The formatting style for command output.

       o json

       o text

       o table

       --query (string)

       A JMESPath query to use in filtering the response data.

       --profile (string)

       Use a specific profile from your credential file.

       --region (string)

       The region to use. Overrides config/env settings.

       --version (string)

       Display the version of this tool.

       --color (string)

       Turn on/off color output.

       o on

       o off

       o auto

       --no-sign-request (boolean)

       Do  not  sign requests. Credentials will not be loaded if this argument
       is provided.

       --ca-bundle (string)

       The CA certificate bundle to use when verifying SSL certificates. Over-
       rides config/env settings.

       --cli-read-timeout (int)

       The  maximum socket read time in seconds. If the value is set to 0, the
       socket read will be blocking and not timeout. The default value  is  60
       seconds.

       --cli-connect-timeout (int)

       The  maximum  socket connect time in seconds. If the value is set to 0,
       the socket connect will be blocking and not timeout. The default  value
       is 60 seconds.

OUTPUT
       id -> (string)
          The unique ID of the new inference endpoint.

       arn -> (string)
          The ARN for the new inference endpoint.

       creationTimeInMillis -> (long)
          The endpoint creation time, in milliseconds.



                                                          CREATE-ML-ENDPOINT()
