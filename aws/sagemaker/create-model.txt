CREATE-MODEL()                                                  CREATE-MODEL()



NAME
       create-model -

DESCRIPTION
       Creates  a  model  in SageMaker. In the request, you name the model and
       describe a primary container. For the primary  container,  you  specify
       the  Docker  image  that contains inference code, artifacts (from prior
       training), and a custom environment map that the  inference  code  uses
       when you deploy the model for predictions.

       Use  this  API  to  create a model if you want to use SageMaker hosting
       services or run a batch transform job.

       To host your model, you create an endpoint configuration with the  Cre-
       ateEndpointConfig  API, and then create an endpoint with the CreateEnd-
       point API. SageMaker then deploys all of the containers  that  you  de-
       fined for the model in the hosting environment.

       For  an  example that calls this method when deploying a model to Sage-
       Maker hosting services, see Create a Model (Amazon Web Services SDK for
       Python (Boto 3)).

       To  run  a  batch  transform using your model, you start a job with the
       CreateTransformJob API. SageMaker uses your model and your  dataset  to
       get inferences which are then saved to a specified S3 location.

       In  the request, you also provide an IAM role that SageMaker can assume
       to access model artifacts and docker image for deployment on ML compute
       hosting  instances  or  for batch transform jobs. In addition, you also
       use the IAM role to manage permissions the inference  code  needs.  For
       example, if the inference code access any other Amazon Web Services re-
       sources, you grant necessary permissions via this role.

       See also: AWS API Documentation

SYNOPSIS
            create-model
          --model-name <value>
          [--primary-container <value>]
          [--containers <value>]
          [--inference-execution-config <value>]
          --execution-role-arn <value>
          [--tags <value>]
          [--vpc-config <value>]
          [--enable-network-isolation | --no-enable-network-isolation]
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]
          [--debug]
          [--endpoint-url <value>]
          [--no-verify-ssl]
          [--no-paginate]
          [--output <value>]
          [--query <value>]
          [--profile <value>]
          [--region <value>]
          [--version <value>]
          [--color <value>]
          [--no-sign-request]
          [--ca-bundle <value>]
          [--cli-read-timeout <value>]
          [--cli-connect-timeout <value>]

OPTIONS
       --model-name (string)
          The name of the new model.

       --primary-container (structure)
          The location of the primary docker image containing inference  code,
          associated  artifacts, and custom environment map that the inference
          code uses when the model is deployed for predictions.

          ContainerHostname -> (string)
              This parameter is ignored for models that contain only a  Prima-
              ryContainer .

              When a ContainerDefinition is part of an inference pipeline, the
              value of the parameter uniquely identifies the container for the
              purposes  of  logging and metrics. For information, see Use Logs
              and Metrics to Monitor an Inference  Pipeline  .  If  you  don't
              specify  a  value  for  this parameter for a ContainerDefinition
              that is part of an inference pipeline, a unique name is automat-
              ically assigned based on the position of the ContainerDefinition
              in the pipeline. If you specify a value for  the  ContainerHost-
              Name  for  any  ContainerDefinition that is part of an inference
              pipeline, you must specify a value for the ContainerHostName pa-
              rameter of every ContainerDefinition in that pipeline.

          Image -> (string)
              The  path  where inference code is stored. This can be either in
              Amazon EC2 Container Registry or in a Docker  registry  that  is
              accessible  from  the  same VPC that you configure for your end-
              point. If you are using your own custom algorithm instead of  an
              algorithm  provided  by  SageMaker, the inference code must meet
              SageMaker requirements. SageMaker supports both registry/reposi-
              tory[:tag]  and registry/repository[@digest] image path formats.
              For more information, see Using Your Own Algorithms with  Amazon
              SageMaker .

              NOTE:
                 The model artifacts in an Amazon S3 bucket and the Docker im-
                 age for inference container in Amazon EC2 Container  Registry
                 must  be  in the same region as the model or endpoint you are
                 creating.

          ImageConfig -> (structure)
              Specifies whether the model container is in Amazon ECR or a pri-
              vate Docker registry accessible from your Amazon Virtual Private
              Cloud (VPC). For information about storing containers in a  pri-
              vate  Docker  registry,  see  Use  a Private Docker Registry for
              Real-Time Inference Containers .

              NOTE:
                 The model artifacts in an Amazon S3 bucket and the Docker im-
                 age  for inference container in Amazon EC2 Container Registry
                 must be in the same region as the model or endpoint  you  are
                 creating.

              RepositoryAccessMode -> (string)
                 Set this to one of the following values:

                 o Platform - The model image is hosted in Amazon ECR.

                 o Vpc  -  The  model image is hosted in a private Docker reg-
                   istry in your VPC.

              RepositoryAuthConfig -> (structure)
                 (Optional) Specifies an authentication configuration for  the
                 private  docker  registry  where  your model image is hosted.
                 Specify a value for this property only if you  specified  Vpc
                 as the value for the RepositoryAccessMode field, and the pri-
                 vate Docker registry where the model image is hosted requires
                 authentication.

                 RepositoryCredentialsProviderArn -> (string)
                     The  Amazon Resource Name (ARN) of an Amazon Web Services
                     Lambda function that provides credentials to authenticate
                     to  the private Docker registry where your model image is
                     hosted. For information about how to create an Amazon Web
                     Services  Lambda  function,  see Create a Lambda function
                     with the console in the Amazon Web Services Lambda Devel-
                     oper Guide .

          Mode -> (string)
              Whether the container hosts a single model or multiple models.

          ModelDataUrl -> (string)
              The  S3  path where the model artifacts, which result from model
              training, are stored. This path must point to a single gzip com-
              pressed  tar  archive  (.tar.gz suffix). The S3 path is required
              for SageMaker built-in algorithms, but not if you use  your  own
              algorithms.  For  more  information  on built-in algorithms, see
              Common Parameters .

              NOTE:
                 The model artifacts must be in an S3 bucket that  is  in  the
                 same region as the model or endpoint you are creating.

              If you provide a value for this parameter, SageMaker uses Amazon
              Web Services Security Token Service to download model  artifacts
              from  the  S3 path you provide. Amazon Web Services STS is acti-
              vated in your Amazon Web Services account  by  default.  If  you
              previously deactivated Amazon Web Services STS for a region, you
              need to reactivate Amazon Web Services STS for that region.  For
              more  information,  see  Activating  and Deactivating Amazon Web
              Services STS in an Amazon Web Services Region in the Amazon  Web
              Services Identity and Access Management User Guide .

              WARNING:
                 If  you use a built-in algorithm to create a model, SageMaker
                 requires that you provide a S3 path to the model artifacts in
                 ModelDataUrl .

          Environment -> (map)
              The  environment  variables to set in the Docker container. Each
              key and value in the Environment string to string map  can  have
              length of up to 1024. We support up to 16 entries in the map.

              key -> (string)

              value -> (string)

          ModelPackageName -> (string)
              The  name  or Amazon Resource Name (ARN) of the model package to
              use to create the model.

          InferenceSpecificationName -> (string)
              The inference specification name in the model package version.

          MultiModelConfig -> (structure)
              Specifies additional configuration for multi-model endpoints.

              ModelCacheSetting -> (string)
                 Whether to cache models for a multi-model  endpoint.  By  de-
                 fault,  multi-model  endpoints  cache  models so that a model
                 does not have to be loaded into memory each time  it  is  in-
                 voked.  Some use cases do not benefit from model caching. For
                 example, if an endpoint hosts a large number of  models  that
                 are  each  invoked  infrequently,  the endpoint might perform
                 better  if  you  disable  model  caching.  To  disable  model
                 caching, set the value of this parameter to Disabled .

       Shorthand Syntax:

          ContainerHostname=string,Image=string,ImageConfig={RepositoryAccessMode=string,RepositoryAuthConfig={RepositoryCredentialsProviderArn=string}},Mode=string,ModelDataUrl=string,Environment={KeyName1=string,KeyName2=string},ModelPackageName=string,InferenceSpecificationName=string,MultiModelConfig={ModelCacheSetting=string}

       JSON Syntax:

          {
            "ContainerHostname": "string",
            "Image": "string",
            "ImageConfig": {
              "RepositoryAccessMode": "Platform"|"Vpc",
              "RepositoryAuthConfig": {
                "RepositoryCredentialsProviderArn": "string"
              }
            },
            "Mode": "SingleModel"|"MultiModel",
            "ModelDataUrl": "string",
            "Environment": {"string": "string"
              ...},
            "ModelPackageName": "string",
            "InferenceSpecificationName": "string",
            "MultiModelConfig": {
              "ModelCacheSetting": "Enabled"|"Disabled"
            }
          }

       --containers (list)
          Specifies the containers in the inference pipeline.

          (structure)
              Describes the container, as part of model definition.

              ContainerHostname -> (string)
                 This parameter is ignored for models that contain only a Pri-
                 maryContainer .

                 When a ContainerDefinition is part of an inference  pipeline,
                 the  value of the parameter uniquely identifies the container
                 for the purposes of logging and metrics. For information, see
                 Use  Logs  and  Metrics to Monitor an Inference Pipeline . If
                 you don't specify a value for this parameter for  a  Contain-
                 erDefinition  that is part of an inference pipeline, a unique
                 name is automatically assigned based on the position  of  the
                 ContainerDefinition  in  the pipeline. If you specify a value
                 for the ContainerHostName for any ContainerDefinition that is
                 part  of  an inference pipeline, you must specify a value for
                 the ContainerHostName parameter of every  ContainerDefinition
                 in that pipeline.

              Image -> (string)
                 The  path  where inference code is stored. This can be either
                 in Amazon EC2 Container Registry or in a Docker registry that
                 is  accessible  from the same VPC that you configure for your
                 endpoint. If you are using your own custom algorithm  instead
                 of  an  algorithm  provided  by SageMaker, the inference code
                 must meet SageMaker  requirements.  SageMaker  supports  both
                 registry/repository[:tag]   and  registry/repository[@digest]
                 image path formats. For more information, see Using Your  Own
                 Algorithms with Amazon SageMaker .

                 NOTE:
                     The model artifacts in an Amazon S3 bucket and the Docker
                     image for inference container  in  Amazon  EC2  Container
                     Registry  must be in the same region as the model or end-
                     point you are creating.

              ImageConfig -> (structure)
                 Specifies whether the model container is in Amazon ECR  or  a
                 private  Docker  registry accessible from your Amazon Virtual
                 Private Cloud (VPC). For information about storing containers
                 in  a  private Docker registry, see Use a Private Docker Reg-
                 istry for Real-Time Inference Containers .

                 NOTE:
                     The model artifacts in an Amazon S3 bucket and the Docker
                     image  for  inference  container  in Amazon EC2 Container
                     Registry must be in the same region as the model or  end-
                     point you are creating.

                 RepositoryAccessMode -> (string)
                     Set this to one of the following values:

                     o Platform - The model image is hosted in Amazon ECR.

                     o Vpc  -  The  model  image is hosted in a private Docker
                       registry in your VPC.

                 RepositoryAuthConfig -> (structure)
                     (Optional) Specifies an authentication configuration  for
                     the  private  docker  registry  where your model image is
                     hosted. Specify a value for this  property  only  if  you
                     specified  Vpc  as the value for the RepositoryAccessMode
                     field, and the private Docker registry  where  the  model
                     image is hosted requires authentication.

                     RepositoryCredentialsProviderArn -> (string)
                        The  Amazon  Resource Name (ARN) of an Amazon Web Ser-
                        vices Lambda function that provides credentials to au-
                        thenticate  to  the private Docker registry where your
                        model image is hosted. For information  about  how  to
                        create  an  Amazon  Web  Services Lambda function, see
                        Create a Lambda function with the console in the  Ama-
                        zon Web Services Lambda Developer Guide .

              Mode -> (string)
                 Whether  the  container hosts a single model or multiple mod-
                 els.

              ModelDataUrl -> (string)
                 The S3 path where the  model  artifacts,  which  result  from
                 model  training, are stored. This path must point to a single
                 gzip compressed tar archive (.tar.gz suffix). The S3 path  is
                 required  for  SageMaker  built-in algorithms, but not if you
                 use your own algorithms. For more information on built-in al-
                 gorithms, see Common Parameters .

                 NOTE:
                     The  model  artifacts  must be in an S3 bucket that is in
                     the same region as the model or endpoint you  are  creat-
                     ing.

                 If  you  provide  a  value for this parameter, SageMaker uses
                 Amazon Web Services Security Token Service to download  model
                 artifacts  from  the S3 path you provide. Amazon Web Services
                 STS is activated in your Amazon Web Services account  by  de-
                 fault.  If you previously deactivated Amazon Web Services STS
                 for a region, you need to reactivate Amazon Web Services  STS
                 for that region. For more information, see Activating and De-
                 activating Amazon Web Services STS in an Amazon Web  Services
                 Region in the Amazon Web Services Identity and Access Manage-
                 ment User Guide .

                 WARNING:
                     If you use a built-in algorithm to create a model,  Sage-
                     Maker  requires  that  you provide a S3 path to the model
                     artifacts in ModelDataUrl .

              Environment -> (map)
                 The environment variables to set  in  the  Docker  container.
                 Each  key  and  value in the Environment string to string map
                 can have length of up to 1024. We support up to 16 entries in
                 the map.

                 key -> (string)

                 value -> (string)

              ModelPackageName -> (string)
                 The  name  or Amazon Resource Name (ARN) of the model package
                 to use to create the model.

              InferenceSpecificationName -> (string)
                 The inference specification name in the  model  package  ver-
                 sion.

              MultiModelConfig -> (structure)
                 Specifies additional configuration for multi-model endpoints.

                 ModelCacheSetting -> (string)
                     Whether  to  cache  models for a multi-model endpoint. By
                     default, multi-model endpoints cache  models  so  that  a
                     model does not have to be loaded into memory each time it
                     is invoked. Some use cases  do  not  benefit  from  model
                     caching. For example, if an endpoint hosts a large number
                     of models that are each invoked  infrequently,  the  end-
                     point  might perform better if you disable model caching.
                     To disable model caching, set the value of this parameter
                     to Disabled .

       Shorthand Syntax:

          ContainerHostname=string,Image=string,ImageConfig={RepositoryAccessMode=string,RepositoryAuthConfig={RepositoryCredentialsProviderArn=string}},Mode=string,ModelDataUrl=string,Environment={KeyName1=string,KeyName2=string},ModelPackageName=string,InferenceSpecificationName=string,MultiModelConfig={ModelCacheSetting=string} ...

       JSON Syntax:

          [
            {
              "ContainerHostname": "string",
              "Image": "string",
              "ImageConfig": {
                "RepositoryAccessMode": "Platform"|"Vpc",
                "RepositoryAuthConfig": {
                  "RepositoryCredentialsProviderArn": "string"
                }
              },
              "Mode": "SingleModel"|"MultiModel",
              "ModelDataUrl": "string",
              "Environment": {"string": "string"
                ...},
              "ModelPackageName": "string",
              "InferenceSpecificationName": "string",
              "MultiModelConfig": {
                "ModelCacheSetting": "Enabled"|"Disabled"
              }
            }
            ...
          ]

       --inference-execution-config (structure)
          Specifies  details  of  how containers in a multi-container endpoint
          are called.

          Mode -> (string)
              How containers in a multi-container are run. The following  val-
              ues are valid.

              o SERIAL - Containers run as a serial pipeline.

              o DIRECT  -  Only  the  individual container that you specify is
                run.

       Shorthand Syntax:

          Mode=string

       JSON Syntax:

          {
            "Mode": "Serial"|"Direct"
          }

       --execution-role-arn (string)
          The Amazon Resource Name (ARN) of the IAM role  that  SageMaker  can
          assume  to access model artifacts and docker image for deployment on
          ML compute instances or for batch transform jobs.  Deploying  on  ML
          compute  instances  is  part of model hosting. For more information,
          see SageMaker Roles .

          NOTE:
              To be able to pass this role to SageMaker, the  caller  of  this
              API must have the iam:PassRole permission.

       --tags (list)
          An  array  of  key-value  pairs. You can use tags to categorize your
          Amazon Web Services resources in different  ways,  for  example,  by
          purpose,  owner,  or  environment. For more information, see Tagging
          Amazon Web Services Resources .

          (structure)
              A tag object that consists of a key and an optional value,  used
              to manage metadata for SageMaker Amazon Web Services resources.

              You  can add tags to notebook instances, training jobs, hyperpa-
              rameter tuning jobs,  batch  transform  jobs,  models,  labeling
              jobs,  work  teams,  endpoint configurations, and endpoints. For
              more information on adding  tags  to  SageMaker  resources,  see
              AddTags .

              For  more information on adding metadata to your Amazon Web Ser-
              vices resources with tagging, see Tagging  Amazon  Web  Services
              resources . For advice on best practices for managing Amazon Web
              Services resources with tagging, see Tagging Best Practices: Im-
              plement an Effective Amazon Web Services Resource Tagging Strat-
              egy .

              Key -> (string)
                 The tag key. Tag keys must be unique per resource.

              Value -> (string)
                 The tag value.

       Shorthand Syntax:

          Key=string,Value=string ...

       JSON Syntax:

          [
            {
              "Key": "string",
              "Value": "string"
            }
            ...
          ]

       --vpc-config (structure)
          A VpcConfig object that specifies the VPC that you want  your  model
          to  connect  to.  Control access to and from your model container by
          configuring the VPC. VpcConfig is used in hosting  services  and  in
          batch  transform. For more information, see Protect Endpoints by Us-
          ing an Amazon Virtual Private Cloud and Protect Data in Batch Trans-
          form Jobs by Using an Amazon Virtual Private Cloud .

          SecurityGroupIds -> (list)
              The VPC security group IDs, in the form sg-xxxxxxxx. Specify the
              security groups for the VPC that is  specified  in  the  Subnets
              field.

              (string)

          Subnets -> (list)
              The  ID  of  the subnets in the VPC to which you want to connect
              your training job or model. For information about the availabil-
              ity of specific instance types, see Supported Instance Types and
              Availability Zones .

              (string)

       Shorthand Syntax:

          SecurityGroupIds=string,string,Subnets=string,string

       JSON Syntax:

          {
            "SecurityGroupIds": ["string", ...],
            "Subnets": ["string", ...]
          }

       --enable-network-isolation | --no-enable-network-isolation (boolean)
          Isolates the model container. No inbound or outbound  network  calls
          can be made to or from the model container.

       --cli-input-json  (string) Performs service operation based on the JSON
       string provided. The JSON string follows the format provided by  --gen-
       erate-cli-skeleton.  If  other  arguments  are  provided on the command
       line, the CLI values will override the JSON-provided values. It is  not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton (string) Prints a  JSON  skeleton  to  standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for  --cli-input-json.  If provided with the value output, it validates
       the command inputs and returns a sample output JSON for that command.

GLOBAL OPTIONS
       --debug (boolean)

       Turn on debug logging.

       --endpoint-url (string)

       Override command's default URL with the given URL.

       --no-verify-ssl (boolean)

       By default, the AWS CLI uses SSL when communicating with AWS  services.
       For each SSL connection, the AWS CLI will verify SSL certificates. This
       option overrides the default behavior of verifying SSL certificates.

       --no-paginate (boolean)

       Disable automatic pagination.

       --output (string)

       The formatting style for command output.

       o json

       o text

       o table

       --query (string)

       A JMESPath query to use in filtering the response data.

       --profile (string)

       Use a specific profile from your credential file.

       --region (string)

       The region to use. Overrides config/env settings.

       --version (string)

       Display the version of this tool.

       --color (string)

       Turn on/off color output.

       o on

       o off

       o auto

       --no-sign-request (boolean)

       Do not sign requests. Credentials will not be loaded if  this  argument
       is provided.

       --ca-bundle (string)

       The CA certificate bundle to use when verifying SSL certificates. Over-
       rides config/env settings.

       --cli-read-timeout (int)

       The maximum socket read time in seconds. If the value is set to 0,  the
       socket  read  will be blocking and not timeout. The default value is 60
       seconds.

       --cli-connect-timeout (int)

       The maximum socket connect time in seconds. If the value is set  to  0,
       the  socket connect will be blocking and not timeout. The default value
       is 60 seconds.

OUTPUT
       ModelArn -> (string)
          The ARN of the model created in SageMaker.



                                                                CREATE-MODEL()
