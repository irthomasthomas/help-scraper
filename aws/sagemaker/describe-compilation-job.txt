DESCRIBE-COMPILATION-JOB()                          DESCRIBE-COMPILATION-JOB()



NAME
       describe-compilation-job -

DESCRIPTION
       Returns information about a model compilation job.

       To  create  a  model compilation job, use CreateCompilationJob . To get
       information   about    multiple    model    compilation    jobs,    use
       ListCompilationJobs .

       See also: AWS API Documentation

SYNOPSIS
            describe-compilation-job
          --compilation-job-name <value>
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]
          [--debug]
          [--endpoint-url <value>]
          [--no-verify-ssl]
          [--no-paginate]
          [--output <value>]
          [--query <value>]
          [--profile <value>]
          [--region <value>]
          [--version <value>]
          [--color <value>]
          [--no-sign-request]
          [--ca-bundle <value>]
          [--cli-read-timeout <value>]
          [--cli-connect-timeout <value>]

OPTIONS
       --compilation-job-name (string)
          The  name  of  the  model  compilation job that you want information
          about.

       --cli-input-json (string) Performs service operation based on the  JSON
       string  provided. The JSON string follows the format provided by --gen-
       erate-cli-skeleton. If other arguments  are  provided  on  the  command
       line,  the CLI values will override the JSON-provided values. It is not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton  (string)  Prints  a  JSON skeleton to standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for --cli-input-json. If provided with the value output,  it  validates
       the command inputs and returns a sample output JSON for that command.

GLOBAL OPTIONS
       --debug (boolean)

       Turn on debug logging.

       --endpoint-url (string)

       Override command's default URL with the given URL.

       --no-verify-ssl (boolean)

       By  default, the AWS CLI uses SSL when communicating with AWS services.
       For each SSL connection, the AWS CLI will verify SSL certificates. This
       option overrides the default behavior of verifying SSL certificates.

       --no-paginate (boolean)

       Disable  automatic pagination. If automatic pagination is disabled, the
       AWS CLI will only make one call, for the first page of results.

       --output (string)

       The formatting style for command output.

       o json

       o text

       o table

       --query (string)

       A JMESPath query to use in filtering the response data.

       --profile (string)

       Use a specific profile from your credential file.

       --region (string)

       The region to use. Overrides config/env settings.

       --version (string)

       Display the version of this tool.

       --color (string)

       Turn on/off color output.

       o on

       o off

       o auto

       --no-sign-request (boolean)

       Do not sign requests. Credentials will not be loaded if  this  argument
       is provided.

       --ca-bundle (string)

       The CA certificate bundle to use when verifying SSL certificates. Over-
       rides config/env settings.

       --cli-read-timeout (int)

       The maximum socket read time in seconds. If the value is set to 0,  the
       socket  read  will be blocking and not timeout. The default value is 60
       seconds.

       --cli-connect-timeout (int)

       The maximum socket connect time in seconds. If the value is set  to  0,
       the  socket connect will be blocking and not timeout. The default value
       is 60 seconds.

OUTPUT
       CompilationJobName -> (string)
          The name of the model compilation job.

       CompilationJobArn -> (string)
          The Amazon Resource Name (ARN) of the model compilation job.

       CompilationJobStatus -> (string)
          The status of the model compilation job.

       CompilationStartTime -> (timestamp)
          The time when the model compilation job started  the  CompilationJob
          instances.

          You are billed for the time between this timestamp and the timestamp
          in the CompilationEndTime field.  In  Amazon  CloudWatch  Logs,  the
          start  time  might  be later than this time. That's because it takes
          time to download the compilation job, which depends on the  size  of
          the compilation job container.

       CompilationEndTime -> (timestamp)
          The  time  when  the  model compilation job on a compilation job in-
          stance ended. For a successful or stopped  job,  this  is  when  the
          job's  model  artifacts  have  finished uploading. For a failed job,
          this is when Amazon SageMaker detected that the job failed.

       StoppingCondition -> (structure)
          Specifies a limit to how long a model compilation job can run.  When
          the  job  reaches the time limit, Amazon SageMaker ends the compila-
          tion job. Use this API to cap model training costs.

          MaxRuntimeInSeconds -> (integer)
              The maximum length of time, in seconds, that a training or  com-
              pilation job can run before it is stopped.

              For  compilation  jobs, if the job does not complete during this
              time, a TimeOut error is generated. We recommend  starting  with
              900 seconds and increasing as necessary based on your model.

              For  all  other  jobs,  if the job does not complete during this
              time, SageMaker ends the job. When RetryStrategy is specified in
              the  job request, MaxRuntimeInSeconds specifies the maximum time
              for all of the attempts in total, not each  individual  attempt.
              The default value is 1 day. The maximum value is 28 days.

              The  maximum time that a TrainingJob can run in total, including
              any time spent publishing metrics  or  archiving  and  uploading
              models after it has been stopped, is 30 days.

          MaxWaitTimeInSeconds -> (integer)
              The  maximum  length  of  time,  in seconds, that a managed Spot
              training job has to complete. It is the  amount  of  time  spent
              waiting  for  Spot  capacity plus the amount of time the job can
              run. It must be equal to or greater than  MaxRuntimeInSeconds  .
              If  the  job  does not complete during this time, SageMaker ends
              the job.

              When RetryStrategy is specified in  the  job  request,  MaxWait-
              TimeInSeconds specifies the maximum time for all of the attempts
              in total, not each individual attempt.

          MaxPendingTimeInSeconds -> (integer)
              The maximum length of time, in seconds, that a training or  com-
              pilation job can be pending before it is stopped.

       InferenceImage -> (string)
          The  inference image to use when compiling a model. Specify an image
          only if the target device is a cloud instance.

       ModelPackageVersionArn -> (string)
          The Amazon Resource Name (ARN) of the versioned model  package  that
          was provided to SageMaker Neo when you initiated a compilation job.

       CreationTime -> (timestamp)
          The time that the model compilation job was created.

       LastModifiedTime -> (timestamp)
          The time that the status of the model compilation job was last modi-
          fied.

       FailureReason -> (string)
          If a model compilation job failed, the reason it failed.

       ModelArtifacts -> (structure)
          Information about the location in Amazon S3 that has been configured
          for storing the model artifacts used in the compilation job.

          S3ModelArtifacts -> (string)
              The path of the S3 object that contains the model artifacts. For
              example, s3://bucket-name/keynameprefix/model.tar.gz .

       ModelDigests -> (structure)
          Provides a BLAKE2 hash value that identifies the compiled model  ar-
          tifacts in Amazon S3.

          ArtifactDigest -> (string)
              Provides  a hash value that uniquely identifies the stored model
              artifacts.

       RoleArn -> (string)
          The Amazon Resource Name (ARN) of an IAM role that Amazon  SageMaker
          assumes to perform the model compilation job.

       InputConfig -> (structure)
          Information about the location in Amazon S3 of the input model arti-
          facts, the name and shape of  the  expected  data  inputs,  and  the
          framework in which the model was trained.

          S3Uri -> (string)
              The  S3  path where the model artifacts, which result from model
              training, are stored. This path must point to a single gzip com-
              pressed tar archive (.tar.gz suffix).

          DataInputConfig -> (string)
              Specifies  the  name  and  shape of the expected data inputs for
              your trained model with a JSON dictionary form. The data  inputs
              are Framework specific.

              o TensorFlow : You must specify the name and shape (NHWC format)
                of the expected data inputs using a dictionary format for your
                trained model. The dictionary formats required for the console
                and CLI are different.

                o Examples for one input:

                  o If using the console, {"input":[1,1024,1024,3]}

                  o If using the CLI, {\"input\":[1,1024,1024,3]}

                o Examples for two inputs:

                  o If   using    the    console,    {"data1":    [1,28,28,1],
                    "data2":[1,28,28,1]}

                  o If     using    the    CLI,    {\"data1\":    [1,28,28,1],
                    \"data2\":[1,28,28,1]}

              o KERAS : You must specify the name and shape (NCHW  format)  of
                expected  data  inputs  using  a  dictionary  format  for your
                trained model. Note that while Keras model artifacts should be
                uploaded in NHWC (channel-last) format, DataInputConfig should
                be specified in NCHW (channel-first)  format.  The  dictionary
                formats required for the console and CLI are different.

                o Examples for one input:

                  o If using the console, {"input_1":[1,3,224,224]}

                  o If using the CLI, {\"input_1\":[1,3,224,224]}

                o Examples for two inputs:

                  o If  using  the  console,  {"input_1":  [1,3,224,224], "in-
                    put_2":[1,3,224,224]}

                  o If  using  the  CLI,  {\"input_1\":  [1,3,224,224],  \"in-
                    put_2\":[1,3,224,224]}

              o MXNET/ONNX/DARKNET : You must specify the name and shape (NCHW
                format) of the expected data inputs in order using  a  dictio-
                nary format for your trained model. The dictionary formats re-
                quired for the console and CLI are different.

                o Examples for one input:

                  o If using the console, {"data":[1,3,1024,1024]}

                  o If using the CLI, {\"data\":[1,3,1024,1024]}

                o Examples for two inputs:

                  o If    using    the    console,    {"var1":    [1,1,28,28],
                    "var2":[1,1,28,28]}

                  o If     using     the    CLI,    {\"var1\":    [1,1,28,28],
                    \"var2\":[1,1,28,28]}

              o PyTorch : You can either specify the name and shape (NCHW for-
                mat)  of expected data inputs in order using a dictionary for-
                mat for your trained model or you can specify the  shape  only
                using  a  list format. The dictionary formats required for the
                console and CLI are different. The list formats for  the  con-
                sole and CLI are the same.

                o Examples for one input in dictionary format:

                  o If using the console, {"input0":[1,3,224,224]}

                  o If using the CLI, {\"input0\":[1,3,224,224]}

                o Example for one input in list format: [[1,3,224,224]]

                o Examples for two inputs in dictionary format:

                  o If   using   the  console,  {"input0":[1,3,224,224],  "in-
                    put1":[1,3,224,224]}

                  o If  using  the   CLI,   {\"input0\":[1,3,224,224],   \"in-
                    put1\":[1,3,224,224]}

                o Example  for  two  inputs  in  list  format: [[1,3,224,224],
                  [1,3,224,224]]

              o XGBOOST : input data name and shape are not needed.
                 DataInputConfig supports the following parameters for  CoreML
                 TargetDevice (ML Model format):

              o shape   :  Input  shape,  for  example  {"input_1":  {"shape":
                [1,224,224,3]}} . In addition to static input  shapes,  CoreML
                converter supports Flexible input shapes:

                o Range  Dimension. You can use the Range Dimension feature if
                  you know the input shape will be within some specific inter-
                  val  in  that  dimension, for example: {"input_1": {"shape":
                  ["1..10", 224, 224, 3]}}

                o Enumerated shapes. Sometimes, the models are trained to work
                  only  on  a select set of inputs. You can enumerate all sup-
                  ported input shapes, for example: {"input_1": {"shape": [[1,
                  224, 224, 3], [1, 160, 160, 3]]}}

              o default_shape  :  Default  input  shape. You can set a default
                shape during conversion for both Range Dimension  and  Enumer-
                ated  Shapes. For example {"input_1": {"shape": ["1..10", 224,
                224, 3], "default_shape": [1, 224, 224, 3]}}

              o type : Input type. Allowed values: Image and Tensor .  By  de-
                fault, the converter generates an ML Model with inputs of type
                Tensor (MultiArray). User can set input type to be Image.  Im-
                age  input  type  requires additional input parameters such as
                bias and scale .

              o bias : If the input type is an Image, you need to provide  the
                bias vector.

              o scale  :  If the input type is an Image, you need to provide a
                scale factor.

              CoreML  ClassifierConfig  parameters  can  be  specified   using
              OutputConfig   CompilerOptions  . CoreML converter supports Ten-
              sorflow and PyTorch models. CoreML conversion examples:

              o Tensor type input:

                o "DataInputConfig":  {"input_1":  {"shape":   [[1,224,224,3],
                  [1,160,160,3]], "default_shape": [1,224,224,3]}}

              o Tensor type input without input name (PyTorch):

                o "DataInputConfig":         [{"shape":        [[1,3,224,224],
                  [1,3,160,160]], "default_shape": [1,3,224,224]}]

              o Image type input:

                o "DataInputConfig":  {"input_1":  {"shape":   [[1,224,224,3],
                  [1,160,160,3]], "default_shape": [1,224,224,3], "type": "Im-
                  age", "bias": [-1,-1,-1], "scale": 0.007843137255}}

                o "CompilerOptions":      {"class_labels":       "imagenet_la-
                  bels_1000.txt"}

              o Image type input without input name (PyTorch):

                o "DataInputConfig":         [{"shape":        [[1,3,224,224],
                  [1,3,160,160]], "default_shape": [1,3,224,224], "type": "Im-
                  age", "bias": [-1,-1,-1], "scale": 0.007843137255}]

                o "CompilerOptions":       {"class_labels":      "imagenet_la-
                  bels_1000.txt"}

              Depending on the model format, DataInputConfig requires the fol-
              lowing parameters for ml_eia2  OutputConfig:TargetDevice .

              o For  TensorFlow models saved in the SavedModel format, specify
                the input names from signature_def_key  and  the  input  model
                shapes  for DataInputConfig . Specify the signature_def_key in
                `                                 OutputConfig:CompilerOptions
                https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#sagemaker-Type-OutputConfig-CompilerOptions`__
                if the model does not use TensorFlow's default  signature  def
                key. For example:

                o "DataInputConfig": {"inputs": [1, 224, 224, 3]}

                o "CompilerOptions": {"signature_def_key": "serving_custom"}

              o For TensorFlow models saved as a frozen graph, specify the in-
                put tensor names and shapes in DataInputConfig and the  output
                tensor  names  for  output_names in ` OutputConfig:CompilerOp-
                tions
                https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_OutputConfig.html#sagemaker-Type-OutputConfig-CompilerOptions`__
                . For example:

                o "DataInputConfig": {"input_tensor:0": [1, 224, 224, 3]}

                o "CompilerOptions": {"output_names": ["output_tensor:0"]}

          Framework -> (string)
              Identifies the framework in which the model was trained. For ex-
              ample: TENSORFLOW.

          FrameworkVersion -> (string)
              Specifies  the  framework version to use. This API field is only
              supported for the MXNet, PyTorch, TensorFlow and TensorFlow Lite
              frameworks.

              For  information  about  framework  versions supported for cloud
              targets and edge devices, see Cloud Supported Instance Types and
              Frameworks and Edge Supported Frameworks .

       OutputConfig -> (structure)
          Information about the output location for the compiled model and the
          target device that the model runs on.

          S3OutputLocation -> (string)
              Identifies the S3 bucket where  you  want  Amazon  SageMaker  to
              store      the      model      artifacts.      For      example,
              s3://bucket-name/key-name-prefix .

          TargetDevice -> (string)
              Identifies the target device or the  machine  learning  instance
              that  you  want  to  run your model on after the compilation has
              completed. Alternatively, you can specify OS, architecture,  and
              accelerator  using TargetPlatform fields. It can be used instead
              of TargetPlatform .

              NOTE:
                 Currently ml_trn1 is available only in US East (N.  Virginia)
                 Region,  and  ml_inf2 is available only in US East (Ohio) Re-
                 gion.

          TargetPlatform -> (structure)
              Contains information about a target platform that you want  your
              model  to run on, such as OS, architecture, and accelerators. It
              is an alternative of TargetDevice .

              The following examples show how to configure the  TargetPlatform
              and CompilerOptions JSON strings for popular target platforms:

              o Raspberry  Pi  3  Model  B+  "TargetPlatform": {"Os": "LINUX",
                "Arch":   "ARM_EABIHF"},       "CompilerOptions":    {'mattr':
                ['+neon']}

              o Jetson TX2  "TargetPlatform": {"Os": "LINUX", "Arch": "ARM64",
                "Accelerator": "NVIDIA"},     "CompilerOptions":  {'gpu-code':
                'sm_62', 'trt-ver': '6.0.1', 'cuda-ver': '10.0'}

              o EC2  m5.2xlarge instance OS  "TargetPlatform": {"Os": "LINUX",
                "Arch": "X86_64",  "Accelerator":  "NVIDIA"},     "CompilerOp-
                tions": {'mcpu': 'skylake-avx512'}

              o RK3399   "TargetPlatform":  {"Os":  "LINUX",  "Arch": "ARM64",
                "Accelerator": "MALI"}

              o ARMv7 phone (CPU)  "TargetPlatform": {"Os": "ANDROID", "Arch":
                "ARM_EABI"},     "CompilerOptions":  {'ANDROID_PLATFORM':  25,
                'mattr': ['+neon']}

              o ARMv8 phone (CPU)  "TargetPlatform": {"Os": "ANDROID", "Arch":
                "ARM64"},    "CompilerOptions": {'ANDROID_PLATFORM': 29}

              Os -> (string)
                 Specifies a target platform OS.

                 o LINUX : Linux-based operating systems.

                 o ANDROID  : Android operating systems. Android API level can
                   be specified using the  ANDROID_PLATFORM  compiler  option.
                   For example, "CompilerOptions": {'ANDROID_PLATFORM': 28}

              Arch -> (string)
                 Specifies a target platform architecture.

                 o X86_64 : 64-bit version of the x86 instruction set.

                 o X86 : 32-bit version of the x86 instruction set.

                 o ARM64 : ARMv8 64-bit CPU.

                 o ARM_EABIHF : ARMv7 32-bit, Hard Float.

                 o ARM_EABI : ARMv7 32-bit, Soft Float. Used by Android 32-bit
                   ARM platform.

              Accelerator -> (string)
                 Specifies a target platform accelerator (optional).

                 o NVIDIA : Nvidia graphics processing unit. It also  requires
                   gpu-code , trt-ver , cuda-ver compiler options

                 o MALI : ARM Mali graphics processor

                 o INTEL_GRAPHICS : Integrated Intel graphics

          CompilerOptions -> (string)
              Specifies  additional  parameters  for  compiler options in JSON
              format. The compiler options are TargetPlatform specific. It  is
              required  for NVIDIA accelerators and highly recommended for CPU
              compilations. For any other cases, it  is  optional  to  specify
              CompilerOptions.

              o DTYPE  : Specifies the data type for the input. When compiling
                for ml_* (except for ml_inf ) instances using  PyTorch  frame-
                work,  provide  the  data  type  (dtype) of the model's input.
                "float32" is used if "DTYPE" is  not  specified.  Options  for
                data type are:

                o float32: Use either "float" or "float32" .

                o int64: Use either "int64" or "long" .

              For example, {"dtype" : "float32"} .

              o CPU  : Compilation for CPU supports the following compiler op-
                tions.

                o mcpu : CPU micro-architecture. For example,  {'mcpu':  'sky-
                  lake-avx512'}

                o mattr   :   CPU  flags.  For  example,  {'mattr':  ['+neon',
                  '+vfpv4']}

              o ARM : Details of ARM CPU compilations.

                o NEON : NEON is an implementation of the Advanced SIMD exten-
                  sion  used  in  ARMv7 processors. For example, add {'mattr':
                  ['+neon']} to the compiler  options  if  compiling  for  ARM
                  32-bit platform with the NEON support.

              o NVIDIA  :  Compilation  for  NVIDIA GPU supports the following
                compiler options.

                o gpu_code : Specifies the targeted architecture.

                o trt-ver : Specifies the TensorRT versions in x.y.z. format.

                o cuda-ver : Specifies the CUDA version in x.y format.

              For   example,   {'gpu-code':   'sm_72',   'trt-ver':   '6.0.1',
              'cuda-ver': '10.1'}

              o ANDROID  : Compilation for the Android OS supports the follow-
                ing compiler options:

                o ANDROID_PLATFORM : Specifies the Android API levels.  Avail-
                  able  levels  range  from  21  to  29.  For  example,  {'AN-
                  DROID_PLATFORM': 28} .

                o mattr : Add {'mattr': ['+neon']} to compiler options if com-
                  piling for ARM 32-bit platform with NEON support.

              o INFERENTIA  : Compilation for target ml_inf1 uses compiler op-
                tions passed in as a JSON string.  For  example,  "CompilerOp-
                tions":  "\"--verbose  1 --num-neuroncores 2 -O2\"" .  For in-
                formation about supported compiler options,  see  Neuron  Com-
                piler CLI Reference Guide .

              o CoreML : Compilation for the CoreML OutputConfig  TargetDevice
                supports the following compiler options:

                o class_labels : Specifies the classification labels file name
                  inside input tar.gz file. For example, {"class_labels": "im-
                  agenet_labels_1000.txt"} . Labels inside the txt file should
                  be separated by newlines.

              o EIA  :  Compilation for the Elastic Inference Accelerator sup-
                ports the following compiler options:

                o precision_mode : Specifies the precision of  compiled  arti-
                  facts.  Supported  values are "FP16" and "FP32" . Default is
                  "FP32" .

                o signature_def_key : Specifies the signature to use for  mod-
                  els  in  SavedModel format. Defaults is TensorFlow's default
                  signature def key.

                o output_names : Specifies a list of output tensor  names  for
                  models in FrozenGraph format. Set at most one API field, ei-
                  ther: signature_def_key or output_names .

              For example: {"precision_mode": "FP32",  "output_names":  ["out-
              put:0"]}

          KmsKeyId -> (string)
              The  Amazon  Web Services Key Management Service key (Amazon Web
              Services KMS) that Amazon SageMaker uses to encrypt your  output
              models  with  Amazon S3 server-side encryption after compilation
              job. If you don't provide a KMS key ID,  Amazon  SageMaker  uses
              the  default  KMS key for Amazon S3 for your role's account. For
              more information, see KMS-Managed Encryption Keys in the  Amazon
              Simple Storage Service Developer Guide.

              The KmsKeyId can be any of the following formats:

              o Key ID: 1234abcd-12ab-34cd-56ef-1234567890ab

              o Key                                                       ARN:
                arn:aws:kms:us-west-2:111122223333:key/1234abcd-12ab-34cd-56ef-1234567890ab

              o Alias name: alias/ExampleAlias

              o Alias name ARN: arn:aws:kms:us-west-2:111122223333:alias/Exam-
                pleAlias

       VpcConfig -> (structure)
          A VpcConfig object that specifies the VPC that you want your  compi-
          lation job to connect to. Control access to your models by configur-
          ing the VPC. For more information, see Protect Compilation  Jobs  by
          Using an Amazon Virtual Private Cloud .

          SecurityGroupIds -> (list)
              The  VPC  security group IDs. IDs have the form of sg-xxxxxxxx .
              Specify the security groups for the VPC that is specified in the
              Subnets field.

              (string)

          Subnets -> (list)
              The  ID  of  the subnets in the VPC that you want to connect the
              compilation job to for accessing the model in Amazon S3.

              (string)

       DerivedInformation -> (structure)
          Information that  SageMaker  Neo  automatically  derived  about  the
          model.

          DerivedDataInputConfig -> (string)
              The  data  input  configuration that SageMaker Neo automatically
              derived for the model. When SageMaker Neo derives this  informa-
              tion,  you  don't  need  to specify the data input configuration
              when you create a compilation job.



                                                    DESCRIBE-COMPILATION-JOB()
