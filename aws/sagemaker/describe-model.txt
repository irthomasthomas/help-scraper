DESCRIBE-MODEL()                                              DESCRIBE-MODEL()



NAME
       describe-model -

DESCRIPTION
       Describes a model that you created using the CreateModel API.

       See also: AWS API Documentation

SYNOPSIS
            describe-model
          --model-name <value>
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]
          [--debug]
          [--endpoint-url <value>]
          [--no-verify-ssl]
          [--no-paginate]
          [--output <value>]
          [--query <value>]
          [--profile <value>]
          [--region <value>]
          [--version <value>]
          [--color <value>]
          [--no-sign-request]
          [--ca-bundle <value>]
          [--cli-read-timeout <value>]
          [--cli-connect-timeout <value>]

OPTIONS
       --model-name (string)
          The name of the model.

       --cli-input-json  (string) Performs service operation based on the JSON
       string provided. The JSON string follows the format provided by  --gen-
       erate-cli-skeleton.  If  other  arguments  are  provided on the command
       line, the CLI values will override the JSON-provided values. It is  not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton (string) Prints a  JSON  skeleton  to  standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for  --cli-input-json.  If provided with the value output, it validates
       the command inputs and returns a sample output JSON for that command.

GLOBAL OPTIONS
       --debug (boolean)

       Turn on debug logging.

       --endpoint-url (string)

       Override command's default URL with the given URL.

       --no-verify-ssl (boolean)

       By default, the AWS CLI uses SSL when communicating with AWS  services.
       For each SSL connection, the AWS CLI will verify SSL certificates. This
       option overrides the default behavior of verifying SSL certificates.

       --no-paginate (boolean)

       Disable automatic pagination. If automatic pagination is disabled,  the
       AWS CLI will only make one call, for the first page of results.

       --output (string)

       The formatting style for command output.

       o json

       o text

       o table

       --query (string)

       A JMESPath query to use in filtering the response data.

       --profile (string)

       Use a specific profile from your credential file.

       --region (string)

       The region to use. Overrides config/env settings.

       --version (string)

       Display the version of this tool.

       --color (string)

       Turn on/off color output.

       o on

       o off

       o auto

       --no-sign-request (boolean)

       Do  not  sign requests. Credentials will not be loaded if this argument
       is provided.

       --ca-bundle (string)

       The CA certificate bundle to use when verifying SSL certificates. Over-
       rides config/env settings.

       --cli-read-timeout (int)

       The  maximum socket read time in seconds. If the value is set to 0, the
       socket read will be blocking and not timeout. The default value  is  60
       seconds.

       --cli-connect-timeout (int)

       The  maximum  socket connect time in seconds. If the value is set to 0,
       the socket connect will be blocking and not timeout. The default  value
       is 60 seconds.

OUTPUT
       ModelName -> (string)
          Name of the SageMaker model.

       PrimaryContainer -> (structure)
          The  location  of  the primary inference code, associated artifacts,
          and custom environment map that the inference code uses when  it  is
          deployed in production.

          ContainerHostname -> (string)
              This  parameter is ignored for models that contain only a Prima-
              ryContainer .

              When a ContainerDefinition is part of an inference pipeline, the
              value of the parameter uniquely identifies the container for the
              purposes of logging and metrics. For information, see  Use  Logs
              and  Metrics  to  Monitor  an  Inference Pipeline . If you don't
              specify a value for this  parameter  for  a  ContainerDefinition
              that is part of an inference pipeline, a unique name is automat-
              ically assigned based on the position of the ContainerDefinition
              in  the  pipeline. If you specify a value for the ContainerHost-
              Name for any ContainerDefinition that is part  of  an  inference
              pipeline, you must specify a value for the ContainerHostName pa-
              rameter of every ContainerDefinition in that pipeline.

          Image -> (string)
              The path where inference code is stored. This can be  either  in
              Amazon  EC2  Container  Registry or in a Docker registry that is
              accessible from the same VPC that you configure  for  your  end-
              point.  If you are using your own custom algorithm instead of an
              algorithm provided by SageMaker, the inference  code  must  meet
              SageMaker requirements. SageMaker supports both registry/reposi-
              tory[:tag] and registry/repository[@digest] image path  formats.
              For  more information, see Using Your Own Algorithms with Amazon
              SageMaker .

              NOTE:
                 The model artifacts in an Amazon S3 bucket and the Docker im-
                 age  for inference container in Amazon EC2 Container Registry
                 must be in the same region as the model or endpoint  you  are
                 creating.

          ImageConfig -> (structure)
              Specifies whether the model container is in Amazon ECR or a pri-
              vate Docker registry accessible from your Amazon Virtual Private
              Cloud  (VPC). For information about storing containers in a pri-
              vate Docker registry, see Use  a  Private  Docker  Registry  for
              Real-Time Inference Containers .

              NOTE:
                 The model artifacts in an Amazon S3 bucket and the Docker im-
                 age for inference container in Amazon EC2 Container  Registry
                 must  be  in the same region as the model or endpoint you are
                 creating.

              RepositoryAccessMode -> (string)
                 Set this to one of the following values:

                 o Platform - The model image is hosted in Amazon ECR.

                 o Vpc - The model image is hosted in a  private  Docker  reg-
                   istry in your VPC.

              RepositoryAuthConfig -> (structure)
                 (Optional)  Specifies an authentication configuration for the
                 private docker registry where your  model  image  is  hosted.
                 Specify  a  value for this property only if you specified Vpc
                 as the value for the RepositoryAccessMode field, and the pri-
                 vate Docker registry where the model image is hosted requires
                 authentication.

                 RepositoryCredentialsProviderArn -> (string)
                     The Amazon Resource Name (ARN) of an Amazon Web  Services
                     Lambda function that provides credentials to authenticate
                     to the private Docker registry where your model image  is
                     hosted. For information about how to create an Amazon Web
                     Services Lambda function, see Create  a  Lambda  function
                     with the console in the Amazon Web Services Lambda Devel-
                     oper Guide .

          Mode -> (string)
              Whether the container hosts a single model or multiple models.

          ModelDataUrl -> (string)
              The S3 path where the model artifacts, which result  from  model
              training, are stored. This path must point to a single gzip com-
              pressed tar archive (.tar.gz suffix). The S3  path  is  required
              for  SageMaker  built-in algorithms, but not if you use your own
              algorithms. For more information  on  built-in  algorithms,  see
              Common Parameters .

              NOTE:
                 The  model  artifacts  must be in an S3 bucket that is in the
                 same region as the model or endpoint you are creating.

              If you provide a value for this parameter, SageMaker uses Amazon
              Web  Services Security Token Service to download model artifacts
              from the S3 path you provide. Amazon Web Services STS  is  acti-
              vated  in  your  Amazon  Web Services account by default. If you
              previously deactivated Amazon Web Services STS for a region, you
              need  to reactivate Amazon Web Services STS for that region. For
              more information, see Activating  and  Deactivating  Amazon  Web
              Services  STS in an Amazon Web Services Region in the Amazon Web
              Services Identity and Access Management User Guide .

              WARNING:
                 If you use a built-in algorithm to create a model,  SageMaker
                 requires that you provide a S3 path to the model artifacts in
                 ModelDataUrl .

          ModelDataSource -> (structure)
              Specifies the location of ML model data to deploy.

              NOTE:
                 Currently you cannot use ModelDataSource in conjunction  with
                 SageMaker  batch  transform,  SageMaker serverless endpoints,
                 SageMaker multi-model endpoints, and SageMaker Marketplace.

              S3DataSource -> (structure)
                 Specifies the S3 location of ML model data to deploy.

                 S3Uri -> (string)
                     Specifies the S3 path of ML model data to deploy.

                 S3DataType -> (string)
                     Specifies the type of ML model data to deploy.

                     If you choose S3Prefix , S3Uri identifies a key name pre-
                     fix.  SageMaker uses all objects that match the specified
                     key name prefix as part of the ML model data to deploy. A
                     valid  key  name  prefix  identified by S3Uri always ends
                     with a forward slash (/).

                     If you choose S3Object , S3Uri identifies an object  that
                     is the ML model data to deploy.

                 CompressionType -> (string)
                     Specifies how the ML model data is prepared.

                     If  you  choose  Gzip and choose S3Object as the value of
                     S3DataType  ,  S3Uri  identifies  an  object  that  is  a
                     gzip-compressed  TAR  archive.  SageMaker will attempt to
                     decompress and untar the object during model deployment.

                     If you choose None and chooose S3Object as the  value  of
                     S3DataType  ,  S3Uri identifies an object that represents
                     an uncompressed ML model to deploy.

                     If you choose None and choose S3Prefix as  the  value  of
                     S3DataType  ,  S3Uri  identifies a key name prefix, under
                     which all objects represents the uncompressed ML model to
                     deploy.

                     If  you choose None, then SageMaker will follow rules be-
                     low when creating model data  files  under  /opt/ml/model
                     directory for use by your inference code:

                     o If  you  choose  S3Object  as the value of S3DataType ,
                       then SageMaker will split the key of the S3 object ref-
                       erenced by S3Uri by slash (/), and use the last part as
                       the filename of the file holding the content of the  S3
                       object.

                     o If  you  choose  S3Prefix  as the value of S3DataType ,
                       then for each S3 object under the key name pefix refer-
                       enced  by  S3Uri  ,  SageMaker will trim its key by the
                       prefix, and use the remainder as the path (relative  to
                       /opt/ml/model  ) of the file holding the content of the
                       S3 object. SageMaker will split the remainder by  slash
                       (/),  using  intermediate  parts as directory names and
                       the last part as filename of the file holding the  con-
                       tent of the S3 object.

                     o Do not use any of the following as file names or direc-
                       tory names:

                       o An empty or blank string

                       o A string which contains null bytes

                       o A string longer than 255 bytes

                       o A single dot (. )

                       o A double dot (.. )

                     o Ambiguous file names will result  in  model  deployment
                       failure.  For  example,  if  your uncompressed ML model
                       consists of two S3 objects  s3://mybucket/model/weights
                       and  s3://mybucket/model/weights/part1  and you specify
                       s3://mybucket/model/ as the value of S3Uri and S3Prefix
                       as  the  value  of  S3DataType , then it will result in
                       name clash  between  /opt/ml/model/weights  (a  regular
                       file) and /opt/ml/model/weights/ (a directory).

                     o Do not organize the model artifacts in S3 console using
                       folders . When you create a folder in  S3  console,  S3
                       creates  a  0-byte  object with a key set to the folder
                       name you provide. They key of the  0-byte  object  ends
                       with  a slash (/) which violates SageMaker restrictions
                       on model artifact file names, leading to model  deploy-
                       ment failure.

                 ModelAccessConfig -> (structure)
                     Specifies the access configuration file for the ML model.
                     You can explicitly  accept  the  model  end-user  license
                     agreement  (EULA)  within the ModelAccessConfig . You are
                     responsible for reviewing and complying with any applica-
                     ble license terms and making sure they are acceptable for
                     your use case before downloading or using a model.

                     AcceptEula -> (boolean)
                        Specifies agreement  to  the  model  end-user  license
                        agreement (EULA). The AcceptEula value must be explic-
                        itly defined as True in order to accept the EULA  that
                        this model requires. You are responsible for reviewing
                        and complying with any applicable  license  terms  and
                        making  sure they are acceptable for your use case be-
                        fore downloading or using a model.

                 HubAccessConfig -> (structure)
                     Configuration information for hub access.

                     HubContentArn -> (string)
                        The ARN of the hub content for which deployment access
                        is allowed.

          AdditionalModelDataSources -> (list)
              Data sources that are available to your model in addition to the
              one that you specify for ModelDataSource when you use  the  Cre-
              ateModel action.

              (structure)
                 Data  sources that are available to your model in addition to
                 the one that you specify for ModelDataSource when you use the
                 CreateModel action.

                 ChannelName -> (string)
                     A custom name for this AdditionalModelDataSource object.

                 S3DataSource -> (structure)
                     Specifies the S3 location of ML model data to deploy.

                     S3Uri -> (string)
                        Specifies the S3 path of ML model data to deploy.

                     S3DataType -> (string)
                        Specifies the type of ML model data to deploy.

                        If  you  choose S3Prefix , S3Uri identifies a key name
                        prefix. SageMaker uses  all  objects  that  match  the
                        specified key name prefix as part of the ML model data
                        to deploy. A valid key name prefix identified by S3Uri
                        always ends with a forward slash (/).

                        If  you  choose  S3Object , S3Uri identifies an object
                        that is the ML model data to deploy.

                     CompressionType -> (string)
                        Specifies how the ML model data is prepared.

                        If you choose Gzip and choose S3Object as the value of
                        S3DataType  ,  S3Uri  identifies  an  object that is a
                        gzip-compressed TAR archive. SageMaker will attempt to
                        decompress  and  untar the object during model deploy-
                        ment.

                        If you choose None and chooose S3Object as  the  value
                        of S3DataType , S3Uri identifies an object that repre-
                        sents an uncompressed ML model to deploy.

                        If you choose None and choose S3Prefix as the value of
                        S3DataType , S3Uri identifies a key name prefix, under
                        which all objects represents the uncompressed ML model
                        to deploy.

                        If  you  choose None, then SageMaker will follow rules
                        below   when   creating   model   data   files   under
                        /opt/ml/model  directory  for  use  by  your inference
                        code:

                        o If you choose S3Object as the value of S3DataType  ,
                          then  SageMaker  will split the key of the S3 object
                          referenced by S3Uri by slash (/), and use  the  last
                          part as the filename of the file holding the content
                          of the S3 object.

                        o If you choose S3Prefix as the value of S3DataType  ,
                          then  for  each  S3  object under the key name pefix
                          referenced by S3Uri , SageMaker will trim its key by
                          the prefix, and use the remainder as the path (rela-
                          tive to /opt/ml/model ) of the file holding the con-
                          tent  of the S3 object. SageMaker will split the re-
                          mainder by slash (/), using  intermediate  parts  as
                          directory names and the last part as filename of the
                          file holding the content of the S3 object.

                        o Do not use any of the following as file names or di-
                          rectory names:

                          o An empty or blank string

                          o A string which contains null bytes

                          o A string longer than 255 bytes

                          o A single dot (. )

                          o A double dot (.. )

                        o Ambiguous file names will result in model deployment
                          failure. For example, if your uncompressed ML  model
                          consists     of     two    S3    objects    s3://my-
                          bucket/model/weights          and           s3://my-
                          bucket/model/weights/part1  and you specify s3://my-
                          bucket/model/ as the value of S3Uri and S3Prefix  as
                          the  value  of  S3DataType  , then it will result in
                          name clash between /opt/ml/model/weights (a  regular
                          file) and /opt/ml/model/weights/ (a directory).

                        o Do  not  organize  the model artifacts in S3 console
                          using folders . When you create a folder in S3  con-
                          sole,  S3  creates a 0-byte object with a key set to
                          the folder name you provide. They key of the  0-byte
                          object  ends  with  a slash (/) which violates Sage-
                          Maker restrictions on  model  artifact  file  names,
                          leading to model deployment failure.

                     ModelAccessConfig -> (structure)
                        Specifies  the  access  configuration  file for the ML
                        model. You can explicitly accept  the  model  end-user
                        license  agreement (EULA) within the ModelAccessConfig
                        . You are responsible for reviewing and complying with
                        any  applicable license terms and making sure they are
                        acceptable for your use case before downloading or us-
                        ing a model.

                        AcceptEula -> (boolean)
                            Specifies  agreement to the model end-user license
                            agreement (EULA). The AcceptEula value must be ex-
                            plicitly  defined  as  True in order to accept the
                            EULA that this model requires. You are responsible
                            for  reviewing  and  complying with any applicable
                            license terms and making sure they are  acceptable
                            for  your  use  case before downloading or using a
                            model.

                     HubAccessConfig -> (structure)
                        Configuration information for hub access.

                        HubContentArn -> (string)
                            The ARN of the hub content  for  which  deployment
                            access is allowed.

          Environment -> (map)
              The  environment variables to set in the Docker container. Don't
              include any sensitive data in your environment variables.

              The maximum length of each key and value in the Environment  map
              is  1024 bytes. The maximum length of all keys and values in the
              map, combined, is 32 KB. If you pass multiple  containers  to  a
              CreateModel  request,  then  the  maximum length of all of their
              maps, combined, is also 32 KB.

              key -> (string)

              value -> (string)

          ModelPackageName -> (string)
              The name or Amazon Resource Name (ARN) of the model  package  to
              use to create the model.

          InferenceSpecificationName -> (string)
              The inference specification name in the model package version.

          MultiModelConfig -> (structure)
              Specifies additional configuration for multi-model endpoints.

              ModelCacheSetting -> (string)
                 Whether  to  cache  models for a multi-model endpoint. By de-
                 fault, multi-model endpoints cache models  so  that  a  model
                 does  not  have  to be loaded into memory each time it is in-
                 voked. Some use cases do not benefit from model caching.  For
                 example,  if  an endpoint hosts a large number of models that
                 are each invoked infrequently,  the  endpoint  might  perform
                 better  if  you  disable  model  caching.  To  disable  model
                 caching, set the value of this parameter to Disabled .

       Containers -> (list)
          The containers in the inference pipeline.

          (structure)
              Describes the container, as part of model definition.

              ContainerHostname -> (string)
                 This parameter is ignored for models that contain only a Pri-
                 maryContainer .

                 When  a ContainerDefinition is part of an inference pipeline,
                 the value of the parameter uniquely identifies the  container
                 for the purposes of logging and metrics. For information, see
                 Use Logs and Metrics to Monitor an Inference  Pipeline  .  If
                 you  don't  specify a value for this parameter for a Contain-
                 erDefinition that is part of an inference pipeline, a  unique
                 name  is  automatically assigned based on the position of the
                 ContainerDefinition in the pipeline. If you specify  a  value
                 for the ContainerHostName for any ContainerDefinition that is
                 part of an inference pipeline, you must specify a  value  for
                 the  ContainerHostName parameter of every ContainerDefinition
                 in that pipeline.

              Image -> (string)
                 The path where inference code is stored. This can  be  either
                 in Amazon EC2 Container Registry or in a Docker registry that
                 is accessible from the same VPC that you configure  for  your
                 endpoint.  If you are using your own custom algorithm instead
                 of an algorithm provided by  SageMaker,  the  inference  code
                 must  meet  SageMaker  requirements.  SageMaker supports both
                 registry/repository[:tag]  and   registry/repository[@digest]
                 image  path formats. For more information, see Using Your Own
                 Algorithms with Amazon SageMaker .

                 NOTE:
                     The model artifacts in an Amazon S3 bucket and the Docker
                     image  for  inference  container  in Amazon EC2 Container
                     Registry must be in the same region as the model or  end-
                     point you are creating.

              ImageConfig -> (structure)
                 Specifies  whether  the model container is in Amazon ECR or a
                 private Docker registry accessible from your  Amazon  Virtual
                 Private Cloud (VPC). For information about storing containers
                 in a private Docker registry, see Use a Private  Docker  Reg-
                 istry for Real-Time Inference Containers .

                 NOTE:
                     The model artifacts in an Amazon S3 bucket and the Docker
                     image for inference container  in  Amazon  EC2  Container
                     Registry  must be in the same region as the model or end-
                     point you are creating.

                 RepositoryAccessMode -> (string)
                     Set this to one of the following values:

                     o Platform - The model image is hosted in Amazon ECR.

                     o Vpc - The model image is hosted  in  a  private  Docker
                       registry in your VPC.

                 RepositoryAuthConfig -> (structure)
                     (Optional)  Specifies an authentication configuration for
                     the private docker registry where  your  model  image  is
                     hosted.  Specify  a  value  for this property only if you
                     specified Vpc as the value for  the  RepositoryAccessMode
                     field,  and  the  private Docker registry where the model
                     image is hosted requires authentication.

                     RepositoryCredentialsProviderArn -> (string)
                        The Amazon Resource Name (ARN) of an Amazon  Web  Ser-
                        vices Lambda function that provides credentials to au-
                        thenticate to the private Docker registry  where  your
                        model  image  is  hosted. For information about how to
                        create an Amazon Web  Services  Lambda  function,  see
                        Create  a Lambda function with the console in the Ama-
                        zon Web Services Lambda Developer Guide .

              Mode -> (string)
                 Whether the container hosts a single model or  multiple  mod-
                 els.

              ModelDataUrl -> (string)
                 The  S3  path  where  the  model artifacts, which result from
                 model training, are stored. This path must point to a  single
                 gzip  compressed tar archive (.tar.gz suffix). The S3 path is
                 required for SageMaker built-in algorithms, but  not  if  you
                 use your own algorithms. For more information on built-in al-
                 gorithms, see Common Parameters .

                 NOTE:
                     The model artifacts must be in an S3 bucket  that  is  in
                     the  same  region as the model or endpoint you are creat-
                     ing.

                 If you provide a value for  this  parameter,  SageMaker  uses
                 Amazon  Web Services Security Token Service to download model
                 artifacts from the S3 path you provide. Amazon  Web  Services
                 STS  is  activated in your Amazon Web Services account by de-
                 fault. If you previously deactivated Amazon Web Services  STS
                 for  a region, you need to reactivate Amazon Web Services STS
                 for that region. For more information, see Activating and De-
                 activating  Amazon Web Services STS in an Amazon Web Services
                 Region in the Amazon Web Services Identity and Access Manage-
                 ment User Guide .

                 WARNING:
                     If  you use a built-in algorithm to create a model, Sage-
                     Maker requires that you provide a S3 path  to  the  model
                     artifacts in ModelDataUrl .

              ModelDataSource -> (structure)
                 Specifies the location of ML model data to deploy.

                 NOTE:
                     Currently  you  cannot use ModelDataSource in conjunction
                     with SageMaker batch transform, SageMaker serverless end-
                     points,  SageMaker  multi-model  endpoints, and SageMaker
                     Marketplace.

                 S3DataSource -> (structure)
                     Specifies the S3 location of ML model data to deploy.

                     S3Uri -> (string)
                        Specifies the S3 path of ML model data to deploy.

                     S3DataType -> (string)
                        Specifies the type of ML model data to deploy.

                        If you choose S3Prefix , S3Uri identifies a  key  name
                        prefix.  SageMaker  uses  all  objects  that match the
                        specified key name prefix as part of the ML model data
                        to deploy. A valid key name prefix identified by S3Uri
                        always ends with a forward slash (/).

                        If you choose S3Object , S3Uri  identifies  an  object
                        that is the ML model data to deploy.

                     CompressionType -> (string)
                        Specifies how the ML model data is prepared.

                        If you choose Gzip and choose S3Object as the value of
                        S3DataType , S3Uri identifies  an  object  that  is  a
                        gzip-compressed TAR archive. SageMaker will attempt to
                        decompress and untar the object during  model  deploy-
                        ment.

                        If  you  choose None and chooose S3Object as the value
                        of S3DataType , S3Uri identifies an object that repre-
                        sents an uncompressed ML model to deploy.

                        If you choose None and choose S3Prefix as the value of
                        S3DataType , S3Uri identifies a key name prefix, under
                        which all objects represents the uncompressed ML model
                        to deploy.

                        If you choose None, then SageMaker will  follow  rules
                        below   when   creating   model   data   files   under
                        /opt/ml/model directory  for  use  by  your  inference
                        code:

                        o If  you choose S3Object as the value of S3DataType ,
                          then SageMaker will split the key of the  S3  object
                          referenced  by  S3Uri by slash (/), and use the last
                          part as the filename of the file holding the content
                          of the S3 object.

                        o If  you choose S3Prefix as the value of S3DataType ,
                          then for each S3 object under  the  key  name  pefix
                          referenced by S3Uri , SageMaker will trim its key by
                          the prefix, and use the remainder as the path (rela-
                          tive to /opt/ml/model ) of the file holding the con-
                          tent of the S3 object. SageMaker will split the  re-
                          mainder  by  slash  (/), using intermediate parts as
                          directory names and the last part as filename of the
                          file holding the content of the S3 object.

                        o Do not use any of the following as file names or di-
                          rectory names:

                          o An empty or blank string

                          o A string which contains null bytes

                          o A string longer than 255 bytes

                          o A single dot (. )

                          o A double dot (.. )

                        o Ambiguous file names will result in model deployment
                          failure.  For example, if your uncompressed ML model
                          consists    of    two    S3     objects     s3://my-
                          bucket/model/weights           and          s3://my-
                          bucket/model/weights/part1 and you specify  s3://my-
                          bucket/model/  as the value of S3Uri and S3Prefix as
                          the value of S3DataType , then  it  will  result  in
                          name  clash between /opt/ml/model/weights (a regular
                          file) and /opt/ml/model/weights/ (a directory).

                        o Do not organize the model artifacts  in  S3  console
                          using  folders . When you create a folder in S3 con-
                          sole, S3 creates a 0-byte object with a key  set  to
                          the  folder name you provide. They key of the 0-byte
                          object ends with a slash (/)  which  violates  Sage-
                          Maker  restrictions  on  model  artifact file names,
                          leading to model deployment failure.

                     ModelAccessConfig -> (structure)
                        Specifies the access configuration  file  for  the  ML
                        model.  You  can  explicitly accept the model end-user
                        license agreement (EULA) within the  ModelAccessConfig
                        . You are responsible for reviewing and complying with
                        any applicable license terms and making sure they  are
                        acceptable for your use case before downloading or us-
                        ing a model.

                        AcceptEula -> (boolean)
                            Specifies agreement to the model end-user  license
                            agreement (EULA). The AcceptEula value must be ex-
                            plicitly defined as True in order  to  accept  the
                            EULA that this model requires. You are responsible
                            for reviewing and complying  with  any  applicable
                            license  terms and making sure they are acceptable
                            for your use case before downloading  or  using  a
                            model.

                     HubAccessConfig -> (structure)
                        Configuration information for hub access.

                        HubContentArn -> (string)
                            The  ARN  of  the hub content for which deployment
                            access is allowed.

              AdditionalModelDataSources -> (list)
                 Data sources that are available to your model in addition  to
                 the one that you specify for ModelDataSource when you use the
                 CreateModel action.

                 (structure)
                     Data sources that are available to your model in addition
                     to  the one that you specify for ModelDataSource when you
                     use the CreateModel action.

                     ChannelName -> (string)
                        A custom name for this  AdditionalModelDataSource  ob-
                        ject.

                     S3DataSource -> (structure)
                        Specifies the S3 location of ML model data to deploy.

                        S3Uri -> (string)
                            Specifies the S3 path of ML model data to deploy.

                        S3DataType -> (string)
                            Specifies the type of ML model data to deploy.

                            If  you  choose  S3Prefix , S3Uri identifies a key
                            name prefix. SageMaker uses all objects that match
                            the  specified  key  name prefix as part of the ML
                            model data to deploy.  A  valid  key  name  prefix
                            identified  by  S3Uri  always  ends with a forward
                            slash (/).

                            If you choose S3Object , S3Uri identifies  an  ob-
                            ject that is the ML model data to deploy.

                        CompressionType -> (string)
                            Specifies how the ML model data is prepared.

                            If  you  choose  Gzip  and  choose S3Object as the
                            value of S3DataType , S3Uri identifies  an  object
                            that  is  a gzip-compressed TAR archive. SageMaker
                            will attempt to decompress and  untar  the  object
                            during model deployment.

                            If  you  choose  None  and chooose S3Object as the
                            value of S3DataType , S3Uri identifies  an  object
                            that  represents  an  uncompressed ML model to de-
                            ploy.

                            If you choose None  and  choose  S3Prefix  as  the
                            value  of S3DataType , S3Uri identifies a key name
                            prefix, under which all objects represents the un-
                            compressed ML model to deploy.

                            If  you  choose  None,  then SageMaker will follow
                            rules below when creating model data  files  under
                            /opt/ml/model  directory for use by your inference
                            code:

                            o If  you  choose  S3Object  as   the   value   of
                              S3DataType  ,  then SageMaker will split the key
                              of the S3 object referenced by  S3Uri  by  slash
                              (/),  and  use  the last part as the filename of
                              the file holding the content of the S3 object.

                            o If  you  choose  S3Prefix  as   the   value   of
                              S3DataType  ,  then for each S3 object under the
                              key name pefix referenced by S3Uri  ,  SageMaker
                              will trim its key by the prefix, and use the re-
                              mainder as the path (relative to /opt/ml/model )
                              of  the  file  holding the content of the S3 ob-
                              ject. SageMaker  will  split  the  remainder  by
                              slash (/), using intermediate parts as directory
                              names and the last part as filename of the  file
                              holding the content of the S3 object.

                            o Do not use any of the following as file names or
                              directory names:

                              o An empty or blank string

                              o A string which contains null bytes

                              o A string longer than 255 bytes

                              o A single dot (. )

                              o A double dot (.. )

                            o Ambiguous file names will result  in  model  de-
                              ployment  failure.  For  example, if your uncom-
                              pressed ML model  consists  of  two  S3  objects
                              s3://mybucket/model/weights     and     s3://my-
                              bucket/model/weights/part1   and   you   specify
                              s3://mybucket/model/  as  the value of S3Uri and
                              S3Prefix as the value of S3DataType  ,  then  it
                              will    result    in    name    clash    between
                              /opt/ml/model/weights  (a  regular   file)   and
                              /opt/ml/model/weights/ (a directory).

                            o Do  not  organize the model artifacts in S3 con-
                              sole using folders . When you create a folder in
                              S3  console,  S3  creates a 0-byte object with a
                              key set to the folder name you provide. They key
                              of the 0-byte object ends with a slash (/) which
                              violates SageMaker restrictions on  model  arti-
                              fact  file  names,  leading  to model deployment
                              failure.

                        ModelAccessConfig -> (structure)
                            Specifies the access configuration file for the ML
                            model.   You   can  explicitly  accept  the  model
                            end-user license agreement (EULA) within the  Mod-
                            elAccessConfig . You are responsible for reviewing
                            and complying with any  applicable  license  terms
                            and  making  sure they are acceptable for your use
                            case before downloading or using a model.

                            AcceptEula -> (boolean)
                               Specifies agreement to the model  end-user  li-
                               cense  agreement  (EULA).  The AcceptEula value
                               must be explicitly defined as True in order  to
                               accept  the  EULA that this model requires. You
                               are responsible  for  reviewing  and  complying
                               with  any  applicable  license terms and making
                               sure they are acceptable for your use case  be-
                               fore downloading or using a model.

                        HubAccessConfig -> (structure)
                            Configuration information for hub access.

                            HubContentArn -> (string)
                               The ARN of the hub content for which deployment
                               access is allowed.

              Environment -> (map)
                 The environment variables to set  in  the  Docker  container.
                 Don't  include  any  sensitive data in your environment vari-
                 ables.

                 The maximum length of each key and value in  the  Environment
                 map  is 1024 bytes. The maximum length of all keys and values
                 in the map, combined, is 32 KB. If you pass multiple contain-
                 ers  to a CreateModel request, then the maximum length of all
                 of their maps, combined, is also 32 KB.

                 key -> (string)

                 value -> (string)

              ModelPackageName -> (string)
                 The name or Amazon Resource Name (ARN) of the  model  package
                 to use to create the model.

              InferenceSpecificationName -> (string)
                 The  inference  specification  name in the model package ver-
                 sion.

              MultiModelConfig -> (structure)
                 Specifies additional configuration for multi-model endpoints.

                 ModelCacheSetting -> (string)
                     Whether to cache models for a  multi-model  endpoint.  By
                     default,  multi-model  endpoints  cache  models so that a
                     model does not have to be loaded into memory each time it
                     is  invoked.  Some  use  cases  do not benefit from model
                     caching. For example, if an endpoint hosts a large number
                     of  models  that  are each invoked infrequently, the end-
                     point might perform better if you disable model  caching.
                     To disable model caching, set the value of this parameter
                     to Disabled .

       InferenceExecutionConfig -> (structure)
          Specifies details of how containers in  a  multi-container  endpoint
          are called.

          Mode -> (string)
              How  containers in a multi-container are run. The following val-
              ues are valid.

              o SERIAL - Containers run as a serial pipeline.

              o DIRECT - Only the individual container  that  you  specify  is
                run.

       ExecutionRoleArn -> (string)
          The  Amazon  Resource  Name (ARN) of the IAM role that you specified
          for the model.

       VpcConfig -> (structure)
          A VpcConfig object that specifies the VPC that this model has access
          to.  For  more information, see Protect Endpoints by Using an Amazon
          Virtual Private Cloud

          SecurityGroupIds -> (list)
              The VPC security group IDs, in the form  sg-xxxxxxxx  .  Specify
              the security groups for the VPC that is specified in the Subnets
              field.

              (string)

          Subnets -> (list)
              The ID of the subnets in the VPC to which you  want  to  connect
              your training job or model. For information about the availabil-
              ity of specific instance types, see Supported Instance Types and
              Availability Zones .

              (string)

       CreationTime -> (timestamp)
          A timestamp that shows when the model was created.

       ModelArn -> (string)
          The Amazon Resource Name (ARN) of the model.

       EnableNetworkIsolation -> (boolean)
          If  True  ,  no  inbound or outbound network calls can be made to or
          from the model container.

       DeploymentRecommendation -> (structure)
          A set of recommended deployment configurations for the model.

          RecommendationStatus -> (string)
              Status of the deployment recommendation. The status NOT_APPLICA-
              BLE  means  that SageMaker is unable to provide a default recom-
              mendation for the model using the information provided.  If  the
              deployment  status  is IN_PROGRESS , retry your API call after a
              few seconds to get a COMPLETED deployment recommendation.

          RealTimeInferenceRecommendations -> (list)
              A list of RealTimeInferenceRecommendation items.

              (structure)
                 The recommended configuration to use for Real-Time Inference.

                 RecommendationId -> (string)
                     The recommendation ID which uniquely identifies each rec-
                     ommendation.

                 InstanceType -> (string)
                     The recommended instance type for Real-Time Inference.

                 Environment -> (map)
                     The recommended environment variables to set in the model
                     container for Real-Time Inference.

                     key -> (string)

                     value -> (string)



                                                              DESCRIBE-MODEL()
