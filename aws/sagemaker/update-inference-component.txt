UPDATE-INFERENCE-COMPONENT()                      UPDATE-INFERENCE-COMPONENT()



NAME
       update-inference-component -

DESCRIPTION
       Updates an inference component.

       See also: AWS API Documentation

SYNOPSIS
            update-inference-component
          --inference-component-name <value>
          [--specification <value>]
          [--runtime-config <value>]
          [--cli-input-json <value>]
          [--generate-cli-skeleton <value>]
          [--debug]
          [--endpoint-url <value>]
          [--no-verify-ssl]
          [--no-paginate]
          [--output <value>]
          [--query <value>]
          [--profile <value>]
          [--region <value>]
          [--version <value>]
          [--color <value>]
          [--no-sign-request]
          [--ca-bundle <value>]
          [--cli-read-timeout <value>]
          [--cli-connect-timeout <value>]

OPTIONS
       --inference-component-name (string)
          The name of the inference component.

       --specification (structure)
          Details about the resources to deploy with this inference component,
          including the model, container, and compute resources.

          ModelName -> (string)
              The name of an existing SageMaker model object in  your  account
              that you want to deploy with the inference component.

          Container -> (structure)
              Defines  a container that provides the runtime environment for a
              model that you deploy with an inference component.

              Image -> (string)
                 The Amazon Elastic Container Registry (Amazon ECR) path where
                 the Docker image for the model is stored.

              ArtifactUrl -> (string)
                 The  Amazon  S3  path where the model artifacts, which result
                 from model training, are stored. This path must  point  to  a
                 single gzip compressed tar archive (.tar.gz suffix).

              Environment -> (map)
                 The  environment  variables  to  set in the Docker container.
                 Each key and value in the  Environment  string-to-string  map
                 can have length of up to 1024. We support up to 16 entries in
                 the map.

                 key -> (string)

                 value -> (string)

          StartupParameters -> (structure)
              Settings that take effect while the model container starts up.

              ModelDataDownloadTimeoutInSeconds -> (integer)
                 The timeout value, in seconds, to download  and  extract  the
                 model  that you want to host from Amazon S3 to the individual
                 inference instance associated with this inference component.

              ContainerStartupHealthCheckTimeoutInSeconds -> (integer)
                 The timeout value, in seconds, for your  inference  container
                 to  pass health check by Amazon S3 Hosting. For more informa-
                 tion about health check, see How Your  Container  Should  Re-
                 spond to Health Check (Ping) Requests .

          ComputeResourceRequirements -> (structure)
              The  compute  resources  allocated  to  run  the model, plus any
              adapter models, that you assign to the inference component.

              Omit this parameter if  your  request  is  meant  to  create  an
              adapter  inference  component. An adapter inference component is
              loaded by a base inference component, and it  uses  the  compute
              resources of the base inference component.

              NumberOfCpuCoresRequired -> (float)
                 The  number  of CPU cores to allocate to run a model that you
                 assign to an inference component.

              NumberOfAcceleratorDevicesRequired -> (float)
                 The number of accelerators to allocate to run  a  model  that
                 you  assign  to  an inference component. Accelerators include
                 GPUs and Amazon Web Services Inferentia.

              MinMemoryRequiredInMb -> (integer)
                 The minimum MB of memory to allocate to run a model that  you
                 assign to an inference component.

              MaxMemoryRequiredInMb -> (integer)
                 The  maximum MB of memory to allocate to run a model that you
                 assign to an inference component.

          BaseInferenceComponentName -> (string)
              The name of an existing inference component that is  to  contain
              the inference component that you're creating with your request.

              Specify  this  parameter only if your request is meant to create
              an adapter inference component. An adapter  inference  component
              contains  the  path  to  an  adapter  model.  The purpose of the
              adapter model is to tailor the inference output of a base  foun-
              dation  model,  which is hosted by the base inference component.
              The adapter inference component uses the compute resources  that
              you assigned to the base inference component.

              When  you  create  an  adapter inference component, use the Con-
              tainer parameter to specify the location of  the  adapter  arti-
              facts.  In the parameter value, use the ArtifactUrl parameter of
              the InferenceComponentContainerSpecification data type.

              Before you can create an adapter inference component,  you  must
              have  an  existing inference component that contains the founda-
              tion model that you want to adapt.

       Shorthand Syntax:

          ModelName=string,Container={Image=string,ArtifactUrl=string,Environment={KeyName1=string,KeyName2=string}},StartupParameters={ModelDataDownloadTimeoutInSeconds=integer,ContainerStartupHealthCheckTimeoutInSeconds=integer},ComputeResourceRequirements={NumberOfCpuCoresRequired=float,NumberOfAcceleratorDevicesRequired=float,MinMemoryRequiredInMb=integer,MaxMemoryRequiredInMb=integer},BaseInferenceComponentName=string

       JSON Syntax:

          {
            "ModelName": "string",
            "Container": {
              "Image": "string",
              "ArtifactUrl": "string",
              "Environment": {"string": "string"
                ...}
            },
            "StartupParameters": {
              "ModelDataDownloadTimeoutInSeconds": integer,
              "ContainerStartupHealthCheckTimeoutInSeconds": integer
            },
            "ComputeResourceRequirements": {
              "NumberOfCpuCoresRequired": float,
              "NumberOfAcceleratorDevicesRequired": float,
              "MinMemoryRequiredInMb": integer,
              "MaxMemoryRequiredInMb": integer
            },
            "BaseInferenceComponentName": "string"
          }

       --runtime-config (structure)
          Runtime settings for a model that is deployed with an inference com-
          ponent.

          CopyCount -> (integer)
              The  number  of  runtime copies of the model container to deploy
              with the inference component. Each copy can serve inference  re-
              quests.

       Shorthand Syntax:

          CopyCount=integer

       JSON Syntax:

          {
            "CopyCount": integer
          }

       --cli-input-json  (string) Performs service operation based on the JSON
       string provided. The JSON string follows the format provided by  --gen-
       erate-cli-skeleton.  If  other  arguments  are  provided on the command
       line, the CLI values will override the JSON-provided values. It is  not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally.

       --generate-cli-skeleton (string) Prints a  JSON  skeleton  to  standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for  --cli-input-json.  If provided with the value output, it validates
       the command inputs and returns a sample output JSON for that command.

GLOBAL OPTIONS
       --debug (boolean)

       Turn on debug logging.

       --endpoint-url (string)

       Override command's default URL with the given URL.

       --no-verify-ssl (boolean)

       By default, the AWS CLI uses SSL when communicating with AWS  services.
       For each SSL connection, the AWS CLI will verify SSL certificates. This
       option overrides the default behavior of verifying SSL certificates.

       --no-paginate (boolean)

       Disable automatic pagination. If automatic pagination is disabled,  the
       AWS CLI will only make one call, for the first page of results.

       --output (string)

       The formatting style for command output.

       o json

       o text

       o table

       --query (string)

       A JMESPath query to use in filtering the response data.

       --profile (string)

       Use a specific profile from your credential file.

       --region (string)

       The region to use. Overrides config/env settings.

       --version (string)

       Display the version of this tool.

       --color (string)

       Turn on/off color output.

       o on

       o off

       o auto

       --no-sign-request (boolean)

       Do  not  sign requests. Credentials will not be loaded if this argument
       is provided.

       --ca-bundle (string)

       The CA certificate bundle to use when verifying SSL certificates. Over-
       rides config/env settings.

       --cli-read-timeout (int)

       The  maximum socket read time in seconds. If the value is set to 0, the
       socket read will be blocking and not timeout. The default value  is  60
       seconds.

       --cli-connect-timeout (int)

       The  maximum  socket connect time in seconds. If the value is set to 0,
       the socket connect will be blocking and not timeout. The default  value
       is 60 seconds.

OUTPUT
       InferenceComponentArn -> (string)
          The Amazon Resource Name (ARN) of the inference component.



                                                  UPDATE-INFERENCE-COMPONENT()
